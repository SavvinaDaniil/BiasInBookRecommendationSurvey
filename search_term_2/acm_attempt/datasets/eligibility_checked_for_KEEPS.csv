Title,Notes,Relevant,Authors,Link,Abstract,Venue,Year,Type
Towards Green Recommender Systems: Investigating the Impact of Data Reduction on Carbon Footprint and Algorithm Performances,"training recommender systems with less data makes the suggestions
more diverse and less biase",Keep,"['Giuseppe Spillo', 'Allegra De Filippo', 'Cataldo Musto', 'Michela Milano', 'Giovanni Semeraro']",https://doi.org/10.1145/3640457.3688160,"This work investigates the path toward green recommender systems by examining the impact of data reduction on both model performance and carbon footprint. In the pursuit of developing energy-efficient recommender systems, we investigated whether and how reducing the training data impacts the performances of several representative recommendation models. In order to obtain a fair comparison, all the models were run based on the implementations available in a popular recommendation library, i.e., RecBole, and used the same experimental settings. Results indicate that: (a) data reduction can be a promising strategy to make recommender systems more sustainable, at the cost of a lower accuracy; (b) training recommender systems with less data makes the suggestions more diverse and less biased. Overall, this study contributes to the ongoing discourse on the development of recommendation models that meet the principles of SDGs, laying the groundwork for the adoption of more sustainable practices in the field.",Proceedings of the 18th ACM Conference on Recommender Systems,2024,Short
Towards Sustainability-aware Recommender Systems: Analyzing the Trade-off Between Algorithms Performance and Carbon Footprint,"results show that more sophisticated approaches
tend to reduce popularity bias (i.e., lower average popularity) and
increase the diversity of the recommendations.",Keep,"['Giuseppe Spillo', 'Allegra De Filippo', 'Cataldo Musto', 'Michela Milano', 'Giovanni Semeraro']",https://doi.org/10.1145/3604915.3608840,"In this paper, we present a comparative analysis of the trade-off between the performance of state-of-the-art recommendation algorithms and their environmental impact. In particular, we compared 18 popular recommendation algorithms in terms of both performance metrics (i.e., accuracy and diversity of the recommendations) as well as in terms of energy consumption and carbon footprint on three different datasets. In order to obtain a fair comparison, all the algorithms were run based on the implementations available in a popular recommendation library, i.e., RecBole, and used the same experimental settings. The outcomes of the experiments showed that the choice of the optimal recommendation algorithm requires a thorough analysis, since more sophisticated algorithms often led to tiny improvements at the cost of an exponential increase of carbon emissions. Through this paper, we aim to shed light on the problem of carbon footprint and energy consumption of recommender systems, and we make the first step towards the development of sustainability-aware recommendation algorithms.",Proceedings of the 17th ACM Conference on Recommender Systems,2023,Short
The Effect of Feedback Granularity on Recommender Systems Performance,higher feedback granularity were often connected with increased Item Coverage and APLT.,Keep,"['Ladislav Peska', 'Stepan Balcar']",https://doi.org/10.1145/3523227.3551479,"The main source of knowledge utilized in recommender systems (RS) is users’ feedback. While the usage of implicit feedback (i.e. user’s behavior statistics) is gaining in prominence, the explicit feedback (i.e. user’s ratings) remain an important data source. This is true especially for domains, where evaluation of an object does not require an extensive usage and users are well motivated to do so (e.g., video-on-demand services or library archives). So far, numerous rating schemes for explicit feedback have been proposed, ranging both in granularity and presentation style. There are several works studying the effect of rating’s scale and presentation on user’s rating behavior, e.g. willingness to provide feedback or various biases in rating behavior. Nonetheless, the effect of ratings granularity on RS performance remain largely under-researched. In this paper, we studied the combined effect of ratings granularity and supposed probability of feedback existence on various performance statistics of recommender systems. Results indicate that decreasing feedback granularity may lead to changes in RS’s performance w.r.t. nDCG for some recommending algorithms. Nonetheless, in most cases the effect of feedback granularity is surpassed by even a small decrease in feedback’s quantity. Therefore, our results corroborate the policy of many major real-world applications, i.e. preference of simpler rating schemes with the higher chance of feedback reception instead of finer-grained rating scenarios.",Proceedings of the 16th ACM Conference on Recommender Systems,2022,Short
Improving Collaborative Metric Learning with Efficient Negative Sampling,"The motivation is that due to the popularity bias in interaction data, popular items tend to be close together. A challenge is thus to push non-matching popular items farther away in the latent space. Spreading popular items apart could then help to reduce the popularity bias often witnessed in recommendation.",Keep,"['Viet-Anh Tran', 'Romain Hennequin', 'Jimena Royo-Letelier', 'Manuel Moussallam']",https://doi.org/10.1145/3331184.3331337,"Distance metric learning based on triplet loss has been applied with success in a wide range of applications such as face recognition, image retrieval, speaker change detection and recently recommendation with the Collaborative Metric Learning (CML) model. However, as we show in this article, CML requires large batches to work reasonably well because of a too simplistic uniform negative sampling strategy for selecting triplets. Due to memory limitations, this makes it difficult to scale in high-dimensional scenarios. To alleviate this problem, we propose here a 2-stage negative sampling strategy which finds triplets that are highly informative for learning. Our strategy allows CML to work effectively in terms of accuracy and popularity bias, even when the batch size is an order of magnitude smaller than what would be needed with the default uniform sampling. We demonstrate the suitability of the proposed strategy for recommendation and exhibit consistent positive results across various datasets.",Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,2019,Short
Modeling the uniqueness of the user preferences for recommendation systems,Using an experimental evaluation with two real datasets we have studied various recommendation strategies and demonstrated the effectiveness of a recommendation strategy that trades between personalization and popularity according to the user’s uniqueness level.,Keep,"['Haggai Roitman', 'David Carmel', 'Yosi Mass', 'Iris Eiron']",https://doi.org/10.1145/2484028.2484102,"In this paper we propose a novel framework for modeling the uniqueness of the user preferences for recommendation systems. User uniqueness is determined by learning to what extent the user's item preferences deviate from those of an ""average user"" in the system. Based on this framework, we suggest three different recommendation strategies that trade between uniqueness and conformity. Using two real item datasets, we demonstrate the effectiveness of our uniqueness based recommendation framework.",Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval,2013,Short
Navigating Serendipity - An Experimental User Study On The Interplay of Trust and Serendipity In Recommender Systems,"Our findings indicate that while interface enhancements did not yield significant increases in trust, they did notably elevate serendipity ratings for previously unknown books.",Keep,"['Irina Nalis', 'Tobias Sippl', 'Thomas Kolb', 'Julia Neidhardt']",https://doi.org/10.1145/3631700.3664901,"Recommender systems play a crucial role in our daily lives, constantly evolving to meet the diverse needs of users. As the pursuit of improved user experiences continues, metrics such as serendipity have emerged within the realm of beyond-accuracy paradigms. However, integrating serendipitous recommendations presents complex challenges, necessitating a delicate balance between novelty, relevance, and user engagement. In this interdisciplinary experimental user study, we address these challenges within the context of a book recommender system. By investigating the impact of interface design changes on user trust, a key determinant of satisfaction with serendipitous recommendations, we measured trust levels for both individual recommended items and the recommender system as a whole. Our findings indicate that while interface enhancements did not yield significant increases in trust, they did notably elevate serendipity ratings for previously unknown books. These results highlight the intricate interplay between technical and psychological factors in the design of recommender systems, emphasizing the importance of human-centered approaches in the creation of more responsible AI applications. This research contributes to ongoing discussions surrounding user-centric recommendation systems and aligns with broader themes of digital humanism and responsible AI.","Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization",2024,Research
Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives,Keep for reference.,Survey,"['Christine Bauer', 'Eva Zangerle', 'Alan Said']",https://doi.org/10.1145/3629170,"Recommender systems research and practice are fast-developing topics with growing adoption in a wide variety of information access scenarios. In this article, we present an overview of research specifically focused on the evaluation of recommender systems. We perform a systematic literature review, in which we analyze 57 papers spanning six years (2017–2022). Focusing on the processes surrounding evaluation, we dial in on the methods applied, the datasets utilized, and the metrics used. Our study shows that the predominant experiment type in research on the evaluation of recommender systems is offline experimentation and that online evaluations are primarily used in combination with other experimentation methods, e.g., an offline experiment. Furthermore, we find that only a few datasets (MovieLens, Amazon review dataset) are widely used, while many datasets are used in only a few papers each. We observe a similar scenario when analyzing the employed performance metrics—a few metrics are widely used (precision, normalized Discounted Cumulative Gain, and Recall), while many others are used in only a few papers. Overall, our review indicates that beyond-accuracy qualities are rarely assessed. Our analysis shows that the research community working on evaluation has focused on the development of evaluation in a rather narrow scope, with the majority of experiments focusing on a few metrics, datasets, and methods.",missing,2024,Research
Rethinking Multi-Interest Learning for Candidate Matching in Recommender Systems,,No,"['Yueqi Xie', 'Jingqi Gao', 'Peilin Zhou', 'Qichen Ye', 'Yining Hua', 'Jae Kim', 'Fangzhao Wu', 'Sunghun Kim']",https://doi.org/10.1145/3604915.3608766,"Existing research efforts for multi-interest candidate matching in recommender systems mainly focus on improving model architecture or incorporating additional information, neglecting the importance of training schemes. This work revisits the training framework and uncovers two major problems hindering the expressiveness of learned multi-interest representations. First, the current training objective (i.e., uniformly sampled softmax) fails to effectively train discriminative representations in a multi-interest learning scenario due to the severe increase in easy negative samples. Second, a routing collapse problem is observed where each learned interest may collapse to express information only from a single item, resulting in information loss. To address these issues, we propose the REMI framework, consisting of an Interest-aware Hard Negative mining strategy (IHN) and a Routing Regularization (RR) method. IHN emphasizes interest-aware hard negatives by proposing an ideal sampling distribution and developing a Monte-Carlo strategy for efficient approximation. RR prevents routing collapse by introducing a novel regularization term on the item-to-interest routing matrices. These two components enhance the learned multi-interest representations from both the optimization objective and the composition information. REMI is a general framework that can be readily applied to various existing multi-interest candidate matching methods. Experiments on three real-world datasets show our method can significantly improve state-of-the-art methods with easy implementation and negligible computational overhead. The source code is available at https://github.com/Tokkiu/REMI.",Proceedings of the 17th ACM Conference on Recommender Systems,2023,Research
On the Effectiveness of Sampled Softmax Loss for Item Recommendation,"In this work, we aim at offering a better understanding of SSM for item recommendation. Specifically, we first theoretically reveal three model-agnostic advantages: (1) mitigating popularity bias, which is beneficial to long-tail recommendation",Keep,"['Jiancan Wu', 'Xiang Wang', 'Xingyu Gao', 'Jiawei Chen', 'Hongcheng Fu', 'Tianyu Qiu']",https://doi.org/10.1145/3637061,"The learning objective plays a fundamental role to build a recommender system. Most methods routinely adopt either pointwise (e.g., binary cross-entropy) or pairwise (e.g., BPR) loss to train the model parameters, while rarely pay attention to softmax loss, which assumes the probabilities of all classes sum up to 1, due to its computational complexity when scaling up to large datasets or intractability for streaming data where the complete item space is not always available. The sampled softmax (SSM) loss emerges as an efficient substitute for softmax loss. Its special case, InfoNCE loss, has been widely used in self-supervised learning and exhibited remarkable performance for contrastive learning. Nonetheless, limited recommendation work uses the SSM loss as the learning objective. Worse still, none of them explores its properties thoroughly and answers “Does SSM loss suit for item recommendation?” and “What are the conceptual advantages of SSM loss, as compared with the prevalent losses?”, to the best of our knowledge.In this work, we aim at offering a better understanding of SSM for item recommendation. Specifically, we first theoretically reveal three model-agnostic advantages: (1) mitigating popularity bias, which is beneficial to long-tail recommendation; (2) mining hard negative samples, which offers informative gradients to optimize model parameters; and (3) maximizing the ranking metric, which facilitates top-K performance. However, based on our empirical studies, we recognize that the default choice of cosine similarity function in SSM limits its ability in learning the magnitudes of representation vectors. As such, the combinations of SSM with the models that also fall short in adjusting magnitudes (e.g., matrix factorization) may result in poor representations. One step further, we provide mathematical proof that message passing schemes in graph convolution networks can adjust representation magnitude according to node degree, which naturally compensates for the shortcoming of SSM. Extensive experiments on four benchmark datasets justify our analyses, demonstrating the superiority of SSM for item recommendation. Our implementations are available in both TensorFlow1 and PyTorch.2",missing,2024,Research
A Survey on Recommender Systems using Graph Neural Network,,Survey,"['Vineeta Anand', 'Ashish Maurya']",https://doi.org/10.1145/3694784,"The expansion of the Internet has resulted in a change in the flow of information. With the vast amount of digital information generated online, it is easy for users to feel overwhelmed. Finding the specific information can be a challenge, and it can be difficult to distinguish credible sources from unreliable ones. This has made recommender system (RS) an integral part of the information services framework. These systems alleviate users from information overload by analyzing users’ past preferences and directing only desirable information toward users. Traditional RSs use approaches like collaborative and content-based filtering to generate recommendations. Recently, these systems have evolved to a whole new level, intuitively optimizing recommendations using deep network models. Graph Neural Networks (GNNs) have become one of the most widely used approaches in RSs, capturing complex relationships between users and items using graphs. In this survey, we provide a literature review of the latest research efforts done on GNN-based RSs. We present an overview of RS, discuss its generalized pipeline and evolution with changing learning approaches. Furthermore, we explore basic GNN architecture and its variants used in RSs, their applications, and some critical challenges for future research.",missing,2024,Research
Modeling User Repeat Consumption Behavior for Online Novel Recommendation,"That is, these models has the popularity bias problem [ 6 ]. Consequently, new novels have slight chances to be recommended, resulting in small new part of MRR@1 scores.",Keep,"['Yuncong Li', 'Cunxiang Yin', 'yancheng he', 'Guoqiang Xu', 'Jing Cai', 'leeven luo', 'Sheng-hua Zhong']",https://doi.org/10.1145/3523227.3546762,"Given a user’s historical interaction sequence, online novel recommendation suggests the next novel the user may be interested in. Online novel recommendation is important but underexplored. In this paper, we concentrate on recommending online novels to new users of an online novel reading platform, whose first visits to the platform occurred in the last seven days. We have two observations about online novel recommendation for new users. First, repeat novel consumption of new users is a common phenomenon. Second, interactions between users and novels are informative. To accurately predict whether a user will reconsume a novel, it is crucial to characterize each interaction at a fine-grained level. Based on these two observations, we propose a neural network for online novel recommendation, called NovelNet. NovelNet can recommend the next novel from both the user’s consumed novels and new novels simultaneously. Specifically, an interaction encoder is used to obtain accurate interaction representation considering fine-grained attributes of interaction, and a pointer network with a pointwise loss is incorporated into NovelNet to recommend previously-consumed novels. Moreover, an online novel recommendation dataset is built from a well-known online novel reading platform and is released for public use as a benchmark. Experimental results on the dataset demonstrate the effectiveness of NovelNet 1.",Proceedings of the 16th ACM Conference on Recommender Systems,2022,Research
A Survey on Stream-Based Recommender Systems,,Survey,"['Marie Al-Ghossein', 'Talel Abdessalem', ""Anthony BARR\\'{E}""]",https://doi.org/10.1145/3453443,"Recommender Systems (RS) have proven to be effective tools to help users overcome information overload, and significant advances have been made in the field over the past two decades. Although addressing the recommendation problem required first a formulation that could be easily studied and evaluated, there currently exists a gap between research contributions and industrial applications where RS are actually deployed. In particular, most RS are meant to function in batch: they rely on a large static dataset and build a recommendation model that is only periodically updated. This functioning introduces several limitations in various settings, leading to considering more realistic settings where RS learn from continuous streams of interactions. Such RS are framed as Stream-Based Recommender Systems (SBRS).In this article, we review SBRS, underline their relation with time-aware RS and online adaptive learning, and present and categorize existing work that tackle the corresponding problem and its multiple facets. We discuss the methodologies used to evaluate SBRS and the adapted datasets that can be used, and finally we outline open challenges in the area.",missing,2021,Research
Graph Masked Autoencoder for Sequential Recommendation,,Keep,"['Yaowen Ye', 'Lianghao Xia', 'Chao Huang']",https://doi.org/10.1145/3539618.3591692,"While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the above issue of heavy reliance on constructing high-quality embedding contrastive views. Instead, an adaptive data reconstruction paradigm is designed to be integrated with the long-range item dependency modeling, for informative augmentation in sequential recommendation. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baseline models and can learn more accurate representations against data noise and sparsity. Our implemented model code is available at https://github.com/HKUDS/MAERec.",Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval,2023,Research
ReuseKNN: Neighborhood Reuse for Differentially Private KNN-Based Recommendations,,Keep,"['Peter M\\""{u}llner', 'Elisabeth Lex', 'Markus Schedl', 'Dominik Kowald']",https://doi.org/10.1145/3608481,"User-based KNN recommender systems (UserKNN) utilize the rating data of a target user’s k nearest neighbors in the recommendation process. This, however, increases the privacy risk of the neighbors, since the recommendations could expose the neighbors’ rating data to other users or malicious parties. To reduce this risk, existing work applies differential privacy by adding randomness to the neighbors’ ratings, which unfortunately reduces the accuracy of UserKNN. In this work, we introduce ReuseKNN, a novel differentially private KNN-based recommender system. The main idea is to identify small but highly reusable neighborhoods so that (i) only a minimal set of users requires protection with differential privacy and (ii) most users do not need to be protected with differential privacy since they are only rarely exploited as neighbors. In our experiments on five diverse datasets, we make two key observations. Firstly, ReuseKNN requires significantly smaller neighborhoods and, thus, fewer neighbors need to be protected with differential privacy compared with traditional UserKNN. Secondly, despite the small neighborhoods, ReuseKNN outperforms UserKNN and a fully differentially private approach in terms of accuracy. Overall, ReuseKNN leads to significantly less privacy risk for users than in the case of UserKNN.",missing,2023,Research
Causality-driven User Modeling for Sequential Recommendations over Time,,Keep,"['Xingming Chen', 'Qing Li']",https://doi.org/10.1145/3589335.3651896,"Contemporary sequential recommendation systems predominantly leverage statistical correlations derived from user interaction histories to predict future preferences. However, these correlations often mask implicit challenges. On the one hand, user data is frequently plagued by implicit, noisy feedback, misdirecting users towards items that fail to align with their actual interests, which is magnified in sequential recommendation contexts. On the other hand, prevalent methods tend to over-rely on similarity-based attention mechanisms across item pairs, which are prone to utilizing heuristic shortcuts, thereby leading to suboptimal recommendation.To tackle these issues, we put forward a causality-driven user modeling approach for sequential recommendation, which pivots towards a causal perspective. Specifically, we involves the application of a causal graph to identify confounding factors that give rise to spurious correlations and to isolate conceptual variables that causally encapsulate user preferences. By learning the representation of these disentangled causal variables at the conceptual level, we can distinguish between causal and non-causal associations while preserving the inherent sequential nature of user behaviors. This enables us to ascertain which elements are critical and which may induce unintended biases. The framework of our method can be compatible with various mainstream sequential models, which offers a robust foundation for reconstructing more accurate and meaningful user and item representations driven by causality.",Companion Proceedings of the ACM Web Conference 2024,2024,Research
Batch-Mix Negative Sampling for Learning Recommendation Retrievers,,Keep,"['Yongfu Fan', 'Jin Chen', 'Yongquan Jiang', 'Defu Lian', 'Fangda Guo', 'Kai Zheng']",https://doi.org/10.1145/3583780.3614789,"Recommendation retrievers commonly retrieve user potentially preferred items from numerous items, where the query and item representation are learned according to the dual encoders with the log-softmax loss. Under real scenarios, the number of items becomes considerably large, making it exceedingly difficult to calculate the partition function with the whole item corpus. Negative sampling, which samples a subset from the item corpus, is widely used to accelerate the model training. Among different samplers, the in-batch sampling is commonly adopted for online recommendation retrievers, which regards the other items within the mini-batch as the negative samples for the given query, owing to its time and memory efficiency. However, the sample selection bias occurs due to the skewed feedback, harming the retrieval quality. In this paper, we propose a negative sampling approach named Batch-Mix Negative Sampling (BMNS), which adopts batch mixing operation to generate additional negatives for model training. Concretely, BMNS first generates new negative items with the sampled mix coefficient from the Beta distribution, after which a tailored correct strategy guided by frequency is designed to match the sampled softmax loss. In this way, the effort of re-encoding items out of the mini-batch is reduced while also improving the representation space of the negative set. The empirical experiments on four real-world datasets demonstrate BMNS is superior to the competitive negative inbatch sampling method.",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,2023,Research
Critique on Natural Noise in Recommender Systems,,Keep,"['Wissam Jurdi', 'Jacques Abdo', 'Jacques Demerjian', 'Abdallah Makhoul']",https://doi.org/10.1145/3447780,"Recommender systems have been upgraded, tested, and applied in many, often incomparable ways. In attempts to diligently understand user behavior in certain environments, those systems have been frequently utilized in domains like e-commerce, e-learning, and tourism. Their increasing need and popularity have allowed the existence of numerous research paths on major issues like data sparsity, cold start, malicious noise, and natural noise, which immensely limit their performance. It is typical that the quality of the data that fuel those systems should be extremely reliable. Inconsistent user information in datasets can alter the performance of recommenders, albeit running advanced personalizing algorithms. The consequences of this can be costly as such systems are employed in abundant online businesses. Successfully managing these inconsistencies results in more personalized user experiences. In this article, the previous works conducted on natural noise management in recommender datasets are thoroughly analyzed. We adequately explore the ways in which the proposed methods measure improved performances and touch on the different natural noise management techniques and the attributes of the solutions. Additionally, we test the evaluation methods employed to assess the approaches and discuss several key gaps and other improvements the field should realize in the future. Our work considers the likelihood of a modern research branch on natural noise management and recommender assessment.",missing,2021,Research
User Cold-start Problem in Multi-armed Bandits: When the First Recommendations Guide the User’s Experience,,Keep,"['Nicollas Silva', 'Thiago Silva', 'Heitor Werneck', 'Leonardo Rocha', 'Adriano Pereira']",https://doi.org/10.1145/3554819,"Nowadays, Recommender Systems have played a crucial role in several entertainment scenarios by making personalised recommendations and guiding the entire users’ journey from their first interaction. Recent works have addressed it as a Contextual Bandit by providing a sequential decision model to explore items not tried yet (or not tried enough) or exploit the best options learned so far. However, this work noticed these current algorithms are limited to naive non-personalised approaches in the first interactions of a new user, offering random or most popular items. Through experiments in three domains, we identify a negative impact of these first choices. Our study indicates that the bandit performance is directly related to the choices made in the first trials. Then, we propose a new approach to balance exploration and exploitation in the first interactions and handle these drawbacks. This approach is based on the Active Learning theory to catch more information about the new users and improve their long-term experience. Our idea is to explore the potential information gain of items that can also please the user’s taste. This method is named Warm-Starting Contextual Bandits, and it statistically outperforms 10 benchmarks in the literature in the long run.",missing,2023,Research
On the instability of embeddings for recommender systems: the case of matrix factorization,,Keep,"['Giovanni Gabbolini', ""Edoardo D'Amico"", 'Cesare Bernardis', 'Paolo Cremonesi']",https://doi.org/10.1145/3412841.3442011,"Most state-of-the-art top-N collaborative recommender systems work by learning embeddings to jointly represent users and items. Learned embeddings are considered to be effective to solve a variety of tasks. Among others, providing and explaining recommendations. In this paper we question the reliability of the embeddings learned by Matrix Factorization (MF). We empirically demonstrate that, by simply changing the initial values assigned to the latent factors, the same MF method generates very different embeddings of items and users, and we highlight that this effect is stronger for less popular items. To overcome these drawbacks, we present a generalization of MF, called Nearest Neighbors Matrix Factorization (NNMF). The new method propagates the information about items and users to their neighbors, speeding up the training procedure and extending the amount of information that supports recommendations and representations. We describe the NNMF variants of three common MF approaches, and with extensive experiments on five different datasets we show that they strongly mitigate the instability issues of the original MF versions and they improve the accuracy of recommendations on the long-tail.",Proceedings of the 36th Annual ACM Symposium on Applied Computing,2021,Research
"A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions",,Keep,"['Tianzi Zang', 'Yanmin Zhu', 'Haobing Liu', 'Ruohan Zhang', 'Jiadi Yu']",https://doi.org/10.1145/3548455,"Traditional recommendation systems are faced with two long-standing obstacles, namely data sparsity and cold-start problems, which promote the emergence and development of Cross-Domain Recommendation (CDR). The core idea of CDR is to leverage information collected from other domains to alleviate the two problems in one domain. Since the early 2010s, many efforts have been engaged for cross-domain recommendation. Recently, with the development of deep learning and neural networks, a large number of methods have emerged. However, there is a limited number of systematic surveys on CDR, especially regarding the latest proposed methods as well as the recommendation scenarios and recommendation tasks they address. In this survey article, we first proposed a two-level taxonomy of cross-domain recommendation that classifies different recommendation scenarios and recommendation tasks. We then introduce and summarize existing cross-domain recommendation approaches under different recommendation scenarios in a structured manner. We also organize datasets commonly used. We conclude this survey by providing several potential research directions about this field.",missing,2022,Research
Inductive Modeling for Realtime Cold Start Recommendations,,Keep,"['Chandler Zuo', 'Jonathan Castaldo', 'Hanqing Zhu', 'Haoyu Zhang', 'Ji Liu', 'Yangpeng Ou', 'Xiao Kong']",https://doi.org/10.1145/3637528.3671588,"In recommendation systems, the timely delivery of new content to their relevant audiences is critical for generating a growing and high quality collection of content for all users. The nature of this problem requires retrieval models to be able to make inferences in real time and with high relevance. There are two specific challenges for cold start contents. First, the information loss problem in a standard Two Tower model, due to the limited feature interactions between the user and item towers, is exacerbated for cold start items due to training data sparsity. Second, the huge volume of user-generated content in industry applications today poses a big bottleneck in the end-to-end latency of recommending new content. To overcome the two challenges, we propose a novel architecture, the Item History Model (IHM). IHM directly injects user-interaction information into the item tower to overcome information loss. In addition, IHM incorporates an inductive structure using attention-based pooling to eliminate the need for recurring training, a key bottleneck for the real-timeness. On both public and industry datasets, we demonstrate that IHM can not only outperform baselines in recommending cold start contents, but also achieves SoTA real-timeness in industry applications.",Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2024,Research
Post-hoc Selection of Pareto-Optimal Solutions in Search and Recommendation,,Keep,"['Vincenzo Paparella', 'Vito Anelli', 'Franco Nardini', 'Raffaele Perego', 'Tommaso Di Noia']",https://doi.org/10.1145/3583780.3615010,"Information Retrieval (IR) and Recommender Systems (RSs) tasks are moving from computing a ranking of final results based on a single metric to multi-objective problems. Solving these problems leads to a set of Pareto-optimal solutions, known as Pareto frontier, in which no objective can be further improved without hurting the others. In principle, all the points on the Pareto frontier are potential candidates to represent the best model selected with respect to the combination of two, or more, metrics. To our knowledge, there are no well-recognized strategies to decide which point should be selected on the frontier in IR and RSs. In this paper, we propose a novel, post-hoc, theoretically-justified technique, named ""Population Distance from Utopia"" (PDU), to identify and select the one-best Pareto-optimal solution. PDU considers fine-grained utopia points, and measures how far each point is from its utopia point, allowing to select solutions tailored to user preferences, a novel feature we call ""calibration"". We compare PDU against state-of-the-art strategies through extensive experiments on tasks from both IR and RS, showing that PDU combined with calibration notably impacts the solution selection.",Proceedings of the 32nd ACM International Conference on Information and Knowledge Management,2023,Research
Disentangled Representation for Diversified Recommendations,,Keep,"['Xiaoying Zhang', 'Hongning Wang', 'Hang Li']",https://doi.org/10.1145/3539597.3570389,"Accuracy and diversity have long been considered to be two conflicting goals for recommendations. We point out, however, that as the diversity is typically measured by certain pre-selected item attributes, e.g., category as the most popularly employed one, improved diversity can be achieved without sacrificing recommendation accuracy, as long as the diversification respects the user's preference about the pre-selected attributes. This calls for a fine-grained understanding of a user's preferences over items, where one needs to recognize the user's choice is driven by the quality of the item itself, or the pre-selected attributes of the item.In this work, we focus on diversity defined on item categories. We propose a general diversification framework agnostic to the choice of recommendation algorithm. Our solution disentangles the learnt user representation in the recommendation module into category- independent and category-dependent components to differentiate a user's preference over items from two orthogonal perspectives. Experimental results on three benchmark datasets and online A/B test demonstrate the effectiveness of our solution in improving both recommendation accuracy and diversity. In-depth analysis suggests that the improvement is due to our improved modeling of users' categorical preferences and refined ranking within item categories.",Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,2023,Research
CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation,,Keep,"['Shengyu Zhang', 'Dong Yao', 'Zhou Zhao', 'Tat-Seng Chua', 'Fei Wu']",https://doi.org/10.1145/3404835.3462908,"Learning user representations based on historical behaviors lies at the core of modern recommender systems. Recent advances in sequential recommenders have convincingly demonstrated high capability in extracting effective user representations from the given behavior sequences. Despite significant progress, we argue that solely modeling the observational behaviors sequences may end up with a brittle and unstable system due to the noisy and sparse nature of user interactions logged. In this paper, we propose to learn accurate and robust user representations, which are required to be less sensitive to (attack on) noisy behaviors and trust more on the indispensable ones, by modeling counterfactual data distribution. Specifically, given an observed behavior sequence, the proposed CauseRec framework identifies dispensable and indispensable concepts at both the fine-grained item level and the abstract interest level. CauseRec conditionally samples user concept sequences from the counterfactual data distributions by replacing dispensable and indispensable concepts within the original concept sequence. With user representations obtained from the synthesized user sequences, CauseRec performs contrastive user representation learning by contrasting the counterfactual with the observational. We conduct extensive experiments on real-world public recommendation benchmarks and justify the effectiveness of CauseRec with multi-aspects model analysis. The results demonstrate that the proposed CauseRec outperforms state-of-the-art sequential recommenders by learning accurate and robust user representations.",Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,2021,Research
Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis,,Keep,"['Vito Anelli', 'Daniele Malitesta', 'Claudio Pomo', 'Alejandro Bellogin', 'Eugenio Di Sciascio', 'Tommaso Di Noia']",https://doi.org/10.1145/3604915.3609489,"The success of graph neural network-based models (GNNs) has significantly advanced recommender systems by effectively modeling users and items as a bipartite, undirected graph. However, many original graph-based works often adopt results from baseline papers without verifying their validity for the specific configuration under analysis. Our work addresses this issue by focusing on the replicability of results. We present a code that successfully replicates results from six popular and recent graph recommendation models (NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark datasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these graph models with traditional collaborative filtering models that historically performed well in offline evaluations. Furthermore, we extend our study to two new datasets (Allrecipes and BookCrossing) that lack established setups in existing literature. As the performance on these datasets differs from the previous benchmarks, we analyze the impact of specific dataset characteristics on recommendation accuracy. By investigating the information flow from users’ neighborhoods, we aim to identify which models are influenced by intrinsic features in the dataset structure. The code to reproduce our experiments is available at:&nbsp;https://github.com/sisinflab/Graph-RSs-Reproducibility.",Proceedings of the 17th ACM Conference on Recommender Systems,2023,Research
Revisiting Neural Retrieval on Accelerators,,Keep,"['Jiaqi Zhai', 'Zhaojie Gong', 'Yueming Wang', 'Xiao Sun', 'Zheng Yan', 'Fu Li', 'Xing Liu']",https://doi.org/10.1145/3580305.3599897,"Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose mixture of logits (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, h-indexer, we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization.",Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2023,Research
Personalized recommendation algorithm of books based on the diffusion of reader's interest,,Keep,['Lei Min'],https://doi.org/10.1145/3573428.3573733,"The ever-growing books help readers acquire knowledge faster than ever before. But the huge scale of these resources also easily makes people fall into the dilemma of ""Information-Explosion"", which prevents the reader from easily locating the books that are really suitable for them. To alleviate this dilemma, we analyzes the principle of the ""Networks-Based-Inference"" algorithm (NBI), which is a classical heuristic algorithm for recommendation. We also proposes an improved algorithm—NBI algorithm using Interest Diffusion (NBI-ID), that derives from this classical algorithm. This improved algorithm inherits the advantages of NBI method in simplicity and effectiveness, and optimizes the allocation of initial information in the process of information diffusion with an interest related indicator. Thus increasing the efficiency of the recommendation results. Experiments on the GoodBooks dataset show that the proposed algorithm improves in accuracy, recall and diversity compared to the classic NBI, CF (Collaborative Filtering) and GRM (Global Ranking Method) algorithms.",Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering,2023,Research
A contextual approach to improve the user's experience in interactive recommendation systems,,Keep,"[""N\\'{\\i}collas Silva"", 'Heitor Werneck', 'Thiago Silva', 'Adriano Pereira', 'Leonardo Rocha']",https://doi.org/10.1145/3470482.3479621,"Recommendation Systems have concerned about the online environment of real-world scenarios where the system should continually learn and predict new recommendations. Current works have handled it as a Multi-Armed Bandit (MAB) problem by proposing parametric bandit models based on the main recommendation concepts to handle the exploitation and exploration dilemma. However, recent works identified a new problem about the way these models handle the user cold-start. Due to the lack of information about the user, these models have intrinsically delivered naive non-personalized recommendations in their first recommendations until the system learns more about the user. The first recommendations of these bandit models are equivalent to a random selection around the items (i.e., a pure-exploration approach) or a biased selection by the most popular items in the system (i.e., a pure-exploitation approach). Thus, to mitigate this problem, we propose a new contextual approach to initialize the bandit models. This context is made by the information available about the items: their popularity and entropy. The idea is to address both exploration and exploitation goals since the first recommendations by mixing entropic and popular items. Indeed, this approach maximizes the user's satisfaction in the long-term run. By a strong experimental evaluation, comparing our proposal with seven state-of-the-art methods in three real datasets, we demonstrate this context achieves statistically significant improvements by outperforming all baselines.",Proceedings of the Brazilian Symposium on Multimedia and the Web,2021,Research
Modeling the Assimilation-Contrast Effects in Online Product Rating Systems: Debiasing and Recommendations,,Keep,"['Xiaoying Zhang', 'Junzhou Zhao', 'John Lui']",https://doi.org/10.1145/3109859.3109885,"The unbiasedness of online product ratings, an important property to ensure that users' ratings indeed reflect their true evaluations to products, is vital both in shaping consumer purchase decisions and providing reliable recommendations. Recent experimental studies showed that distortions from historical ratings would ruin the unbiasedness of subsequent ratings. How to ""discover"" the distortions from historical ratings in each single rating (or at the micro-level), and perform the ""debiasing operations"" in real rating systems are the main objectives of this work.Using 42 million real customer ratings, we first show that users either ""assimilate"" or ""contrast"" to historical ratings under different scenarios: users conform to historical ratings if historical ratings are not far from the product quality (assimilation), while users deviate from historical ratings if historical ratings are significantly different from the product quality (contrast). This phenomenon can be explained by the well-known psychological argument: the ""Assimilate-Contrast"" theory. However, none of the existing works on modeling historical ratings' influence have taken this into account, and this motivates us to propose the Historical Influence Aware Latent Factor Model (HIALF), the first model for real rating systems to capture and mitigate historical distortions in each single rating. HIALF also allows us to study the influence patterns of historical ratings from a modeling perspective, and it perfectly matches the assimilation and contrast effects we previously observed. Also, HIALF achieves significant improvements in predicting subsequent ratings, and accurately predicts the relationships revealed in previous empirical measurements on real ratings. Finally, we show that HIALF can contribute to better recommendations by decoupling users' real preference from distorted ratings, and reveal the intrinsic product quality for wiser consumer purchase decisions.",Proceedings of the Eleventh ACM Conference on Recommender Systems,2017,Research
Unifying Explicit and Implicit Feedback for Rating Prediction and Ranking Recommendation Tasks,,Keep,"['Amir Jadidinejad', 'Craig Macdonald', 'Iadh Ounis']",https://doi.org/10.1145/3341981.3344225,"The two main tasks addressed by collaborative filtering approaches are rating prediction and ranking. Rating prediction models leverage explicit feedback (e.g. ratings), and aim to estimate the rating a user would assign to an unseen item. In contrast, ranking models leverage implicit feedback (e.g. clicks) in order to provide the user with a personalized ranked list of recommended items. Several previous approaches have been proposed that learn from both explicit and implicit feedback to optimize the task of ranking or rating prediction at the level of recommendation algorithm. Yet we argue that these two tasks are not completely separate, but are part of a unified process: a user first interacts with a set of items and then might decide to provide explicit feedback on a subset of items. We propose to bridge the gap between the tasks of rating prediction and ranking through the use of a novel weak supervision approach that unifies both explicit and implicit feedback datasets. The key aspects of the proposed model is that (1) it is applied at the level of data pre-processing and (2) it increases the representation of less popular items in recommendations while maintaining reasonable recommendation performance. Our experimental results - on six datasets covering different types of heterogeneous user's interactions and using a wide range of evaluation metrics - show that, our proposed approach can effectively combine explicit and implicit feedback and improve the effectiveness of the baseline explicit model on the ranking task by covering a broader range of long-tail items.",Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval,2019,Research
"Updatable, Accurate, Diverse, and Scalable Recommendations for Interactive Applications",,Keep,"['Bibek Paudel', 'Fabian Christoffel', 'Chris Newell', 'Abraham Bernstein']",https://doi.org/10.1145/2955101,"Recommender systems form the backbone of many interactive systems. They incorporate user feedback to personalize the user experience typically via personalized recommendation lists. As users interact with a system, an increasing amount of data about a user’s preferences becomes available, which can be leveraged for improving the systems’ performance. Incorporating these new data into the underlying recommendation model is, however, not always straightforward. Many models used by recommender systems are computationally expensive and, therefore, have to perform offline computations to compile the recommendation lists. For interactive applications, it is desirable to be able to update the computed values as soon as new user interaction data is available: updating recommendations in interactive time using new feedback data leads to better accuracy and increases the attraction of the system to the users. Additionally, there is a growing consensus that accuracy alone is not enough and user satisfaction is also dependent on diverse recommendations.In this work, we tackle this problem of updating personalized recommendation lists for interactive applications in order to provide both accurate and diverse recommendations. To that end, we explore algorithms that exploit random walks as a sampling technique to obtain diverse recommendations without compromising on efficiency and accuracy. Specifically, we present a novel graph vertex ranking recommendation algorithm called RP3β that reranks items based on three-hop random walk transition probabilities. We show empirically that RP3β provides accurate recommendations with high long-tail item frequency at the top of the recommendation list. We also present approximate versions of RP3β and the two most accurate previously published vertex ranking algorithms based on random walk transition probabilities and show that these approximations converge with an increasing number of samples.To obtain interactively updatable recommendations, we additionally show how our algorithm can be extended for online updates at interactive speeds. The underlying random walk sampling technique makes it possible to perform the updates without having to recompute the values for the entire dataset.In an empirical evaluation with three real-world datasets, we show that RP3β provides highly accurate and diverse recommendations that can easily be updated with newly gathered information at interactive speeds (≪ 100ms).",missing,2016,Research
Improving Graph Collaborative Filtering with Directional Behavior Enhanced Contrastive Learning,,Keep,"['Penghang Yu', 'Bing-Kun Bao', 'Zhiyi Tan', 'Guanming Lu']",https://doi.org/10.1145/3663574,"Graph Collaborative Filtering is a widely adopted approach for recommendation, which captures similar behavior features through Graph Neural Network (GNN). Recently, Contrastive Learning (CL) has been demonstrated as an effective method to enhance the performance of graph collaborative filtering. Typically, CL-based methods first perturb users’ history behavior data (e.g., drop clicked items), then construct a self-discriminating task for behavior representations under different random perturbations. However, for widely existing inactive users, random perturbation makes their sparse behavior information more incomplete, thereby harming the behavior feature extraction. To tackle the above issue, we design a novel directional perturbation-based CL method to improve the graph collaborative filtering performance. The idea is to perturb node representations through directionally enhancing behavior features. To do so, we propose a simple yet effective feedback mechanism, which fuses the representations of nodes based on behavior similarity. Then, to avoid irrelevant behavior preferences introduced by the feedback mechanism, we construct a behavior self-contrast task before and after feedback, to align the node representations between the final output and the first layer of GNN. Different from the widely adopted self-discriminating task, the behavior self-contrast task avoids complex message propagation on different perturbed graphs, which is more efficient than previous methods. Extensive experiments on three public datasets demonstrate that the proposed method has distinct advantages over other CL methods on recommendation accuracy.",missing,2024,Research
"Blockbusters and Wallflowers: Accurate, Diverse, and Scalable Recommendations with Random Walks",,Keep,"['Fabian Christoffel', 'Bibek Paudel', 'Chris Newell', 'Abraham Bernstein']",https://doi.org/10.1145/2792838.2800180,"User satisfaction is often dependent on providing accurate and diverse recommendations. In this paper, we explore scalable algorithms that exploit random walks as a sampling technique to obtain diverse recommendations without compromising on accuracy. Specifically, we present a novel graph vertex ranking recommendation algorithm called RP^3_beta that re-ranks items based on 3-hop random walk transition probabilities. We show empirically, that RP^3_beta provides accurate recommendations with high long-tail item frequency at the top of the recommendation list. We also present scalable approximate versions of RP^3_beta and the two most accurate previously published vertex ranking algorithms based on random walk transition probabilities and show that these approximations converge with increasing number of samples.",Proceedings of the 9th ACM Conference on Recommender Systems,2015,Research
Triangle Graph Interest Network for Click-through Rate Prediction,,Keep,"['Wensen Jiang', 'Yizhu Jiao', 'Qingqin Wang', 'Chuanming Liang', 'Lijie Guo', 'Yao Zhang', 'Zhijun Sun', 'Yun Xiong', 'Yangyong Zhu']",https://doi.org/10.1145/3488560.3498458,"Click-through rate prediction is a critical task in online advertising. Currently, many existing methods attempt to extract user potential interests from historical click behavior sequences. However, it is difficult to handle sparse user behaviors or broaden interest exploration. Recently, some researchers incorporate the item-item co-occurrence graph as an auxiliary. Due to the elusiveness of user interests, those works still fail to determine the real motivation of user click behaviors. Besides, those works are more biased towards popular or similar commodities. They lack an effective mechanism to break the diversity restrictions. In this paper, we point out two special properties of triangles in the item-item graphs for recommendation systems: Intra-triangle homophily and Inter-triangle heterophiy. Based on this, we propose a novel and effective framework named Triangle Graph Interest Network (TGIN). For each clicked item in user behavior sequences, we introduce the triangles in its neighborhood of the item-item graphs as a supplement. TGIN regards these triangles as the basic units of user interests, which provide the clues to capture the real motivation for a user clicking an item. We characterize every click behavior by aggregating the information of several interest units to alleviate the elusive motivation problem. The attention mechanism determines users' preference for different interest units. By selecting diverse and relative triangles, short brings in novel and serendipitous items to expand exploration opportunities of user interests. Then, we aggregate the multi-level interests of historical behavior sequences to improve CTR prediction. Extensive experiments on both of public and industrial datasets clearly verify the effectiveness of our framework.",Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,2022,Research
"Diversity, Serendipity, Novelty, and Coverage: A Survey and Empirical Analysis of Beyond-Accuracy Objectives in Recommender Systems",,Keep,"['Marius Kaminskas', 'Derek Bridge']",https://doi.org/10.1145/2926720,"What makes a good recommendation or good list of recommendations?Research into recommender systems has traditionally focused on accuracy, in particular how closely the recommender’s predicted ratings are to the users’ true ratings. However, it has been recognized that other recommendation qualities—such as whether the list of recommendations is diverse and whether it contains novel items—may have a significant impact on the overall quality of a recommender system. Consequently, in recent years, the focus of recommender systems research has shifted to include a wider range of “beyond accuracy” objectives.In this article, we present a survey of the most discussed beyond-accuracy objectives in recommender systems research: diversity, serendipity, novelty, and coverage. We review the definitions of these objectives and corresponding metrics found in the literature. We also review works that propose optimization strategies for these beyond-accuracy objectives. Since the majority of works focus on one specific objective, we find that it is not clear how the different objectives relate to each other.Hence, we conduct a set of offline experiments aimed at comparing the performance of different optimization approaches with a view to seeing how they affect objectives other than the ones they are optimizing. We use a set of state-of-the-art recommendation algorithms optimized for recall along with a number of reranking strategies for optimizing the diversity, novelty, and serendipity of the generated recommendations. For each reranking strategy, we measure the effects on the other beyond-accuracy objectives and demonstrate important insights into the correlations between the discussed objectives. For instance, we find that rating-based diversity is positively correlated with novelty, and we demonstrate the positive influence of novelty on recommendation coverage.",missing,2016,Research
"Complexities associated with user-generated book reviews in digital libraries: temporal, cultural, and political case studies",,Keep,"['Yuerong Hu', 'Zoe LeBlanc', 'Jana Diesner', 'Ted Underwood', 'Glen Layne-Worthey', 'J. Downie']",https://doi.org/10.1145/3529372.3530930,"While digital libraries (DL) have made large-scale collections of digitized books increasingly available to researchers [31, 67], there remains a dearth of similar data provisions or infrastructure for computational studies of the consumption and reception of books. In the last two decades, user-generated book reviews on social media have opened up unprecedented research possibilities for humanities and social sciences (HSS) scholars who are interested in book reception. However, limitations and gaps have emerged from existing DH research which utilize social media data for answering HSS questions. To shed light on the under-investigated features of user-generated book reviews and the challenges they might pose to scholarly research, we conducted three exemplar cases studies: (1) a longitudinal analysis for profiling the temporal changes of ratings and popularity of 552 books across ten years; (2) a cross-cultural comparison of book ratings of the same 538 books across two platforms; and, (3) a classification experiment on 20,000 sponsored and non-sponsored books reviews. Correspondingly, our research reveals the real-world complexities and under-investigated features of user-generated book reviews in three dimensions: the transience of book ratings and popularity (temporal dimension), the cross-cultural differences in reading interests and book reception (cultural dimension), and the user power dynamics behind the publicly accessible reviews (""political"" dimension). Our case studies also demonstrate the challenges posed by user-generated book reviews' real-world complexities to their scholarly usage and propose solutions to these challenges. We conclude that DL stakeholders and scholars working with user-generated book reviews should look into these under-investigated features and real-world challenges to evaluate and improve the scholarly usability and interpretability of their data.",Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries,2022,Research
HybridSVD: when collaborative information is not enough,,Keep,"['Evgeny Frolov', 'Ivan Oseledets']",https://doi.org/10.1145/3298689.3347055,"We propose a new hybrid algorithm that allows incorporating both user and item side information within the standard collaborative filtering technique. One of its key features is that it naturally extends a simple PureSVD approach and inherits its unique advantages, such as highly efficient Lanczos-based optimization procedure, simplified hyper-parameter tuning and a quick folding-in computation for generating recommendations instantly even in highly dynamic online environments. The algorithm utilizes a generalized formulation of the singular value decomposition, which adds flexibility to the solution and allows imposing the desired structure on its latent space. Conveniently, the resulting model also admits an efficient and straightforward solution for the cold start scenario. We evaluate our approach on a diverse set of datasets and show its superiority over similar classes of hybrid models.",Proceedings of the 13th ACM Conference on Recommender Systems,2019,Research
"Understanding Assimilation-contrast Effects in Online Rating Systems: Modelling, Debiasing, and Applications",,Keep,"['Xiaoying Zhang', 'Hong Xie', 'Junzhou Zhao', 'John Lui']",https://doi.org/10.1145/3362651,"“Unbiasedness,” which is an important property to ensure that users’ ratings indeed reflect their true evaluations of products, is vital both in shaping consumer purchase decisions and providing reliable recommendations in online rating systems. Recent experimental studies showed that distortions from historical ratings would ruin the unbiasedness of subsequent ratings. How to “discover” historical distortions in each single rating (or at the micro-level), and perform the “debiasing operations” are our main objective. Using 42M real customer ratings, we first show that users either “assimilate” or “contrast” to historical ratings under different scenarios, which can be further explained by a well-known psychological argument: the “Assimilate-Contrast” theory. This motivates us to propose the Historical Influence Aware Latent Factor Model (HIALF), the “first” model for real rating systems to capture and mitigate historical distortions in each single rating. HIALF allows us to study the influence patterns of historical ratings from a modelling perspective, which perfectly matches the assimilation and contrast effects observed in experiments. Moreover, HIALF achieves significant improvements in predicting subsequent ratings and characterizing relationships in ratings. It also contributes to better recommendations, wiser consumer purchase decisions, and deeper understanding of historical distortions in both honest rating and misbehaving rating settings.",missing,2019,Research
"Fewer Flops at the Top: Accuracy, Diversity, and Regularization in Two-Class Collaborative Filtering",,Keep,"['Bibek Paudel', 'Thilo Haas', 'Abraham Bernstein']",https://doi.org/10.1145/3109859.3109916,"In most existing recommender systems, implicit or explicit interactions are treated as positive links and all unknown interactions are treated as negative links. The goal is to suggest new links that will be perceived as positive by users. However, as signed social networks and newer content services become common, it is important to distinguish between positive and negative preferences. Even in existing applications, the cost of a negative recommendation could be high when people are looking for new jobs, friends, or places to live.In this work, we develop novel probabilistic latent factor models to recommend positive links and compare them with existing methods on five different openly available datasets. Our models are able to produce better ranking lists and are effective in the task of ranking positive links at the top, with fewer negative links (flops). Moreover, we find that modeling signed social networks and user preferences this way has the advantage of increasing the diversity of recommendations. We also investigate the effect of regularization on the quality of recommendations, a matter that has not received enough attention in the literature. We find that regularization parameter heavily affects the quality of recommendations in terms of both accuracy and diversity.",Proceedings of the Eleventh ACM Conference on Recommender Systems,2017,Research
Offline Retrieval Evaluation Without Evaluation Metrics,,Keep,"['Fernando Diaz', 'Andres Ferraro']",https://doi.org/10.1145/3477495.3532033,"Offline evaluation of information retrieval and recommendation has traditionally focused on distilling the quality of a ranking into a scalar metric such as average precision or normalized discounted cumulative gain. We can use this metric to compare the performance of multiple systems for the same request. Although evaluation metrics provide a convenient summary of system performance, they also collapse subtle differences across users into a single number and can carry assumptions about user behavior and utility not supported across retrieval scenarios. We propose recall-paired preference (RPP), a metric-free evaluation method based on directly computing a preference between ranked lists. RPP simulates multiple user subpopulations per query and compares systems across these pseudo-populations. Our results across multiple search and recommendation tasks demonstrate that RPP substantially improves discriminative power while correlating well with existing metrics and being equally robust to incomplete data.",Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,2022,Research
Spectral Relaxations and Fair Densest Subgraphs,,Keep,"['Aris Anagnostopoulos', 'Luca Becchetti', 'Adriano Fazzone', 'Cristina Menghini', 'Chris Schwiegelshohn']",https://doi.org/10.1145/3340531.3412036,"Reducing hidden bias in the data and ensuring fairness in algorithmic data analysis has recently received significant attention. In this paper, we address the problem of identifying a densest subgraph, while ensuring that none of one binary protected attribute is disparately impacted.Unfortunately, the underlying algorithmic problem is NP-hard, even in its approximation version: approximating the densest fair subgraph with a polynomial-time algorithm is at least as hard as the densest subgraph problem of at most k vertices, for which no constant approximation algorithms are known.Despite such negative premises, we are able to provide approximation results in two important cases. In particular, we are able to prove that a suitable spectral embedding allows recovery of an almost optimal, fair, dense subgraph hidden in the input data, whenever one is present, a result that is further supported by experimental evidence. We also show a polynomial-time, $2$-approximation algorithm, whenever the underlying graph is itself fair. We finally prove that, under the small set expansion hypothesis, this result is tight for fair graphs.The above theoretical findings drive the design of heuristics, which we experimentally evaluate on a scenario based on real data, in which our aim is to strike a good balance between diversity and highly correlated items from Amazon co-purchasing graphs.",Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management,2020,Research
Notes toward a politics of personalization,,Keep,['Michael Murphy'],https://doi.org/10.1145/1940761.1940836,"A recommender system is an information organization tool which extracts knowledge of individual users of a specific (online) resource based on their activity within that domain, and uses this knowledge to generate for them individual recommendations. These recommendations are made based on the broad assumption that people who have agreed on some things in the past will likely agree on things in the future. [1] Because these systems classify content based on how it is engaged with by previous users, they have emerged as an effective (and profitable) way to organize content on the web, the web itself being resistant, almost by its very nature, to the imposition of top-down, ontological classificatory control. [2]This paper interrogates some of the assumptions and biases involved in the personalized organization and presentation of digital media content, in an effort to devise a more critical analysis of the economic, technical, and social forces that contribute to the growing popularity of the personalized recommender systems. It pays particular attention to how personalized content filtering finds expression in the recommendation engines of taste-coordinating websites like Amazon and Pandora, in the self-organization of information through social classification sites like LibraryThing and Delicious, in the adaptive capabilities of next generation search engines (what Michael Zimmer refers to as ""Search 2.0""), and finally within locational media software like FourSquare. Discussion of these recommender systems will refer to their mechanics as well as their accompanying rhetoric, which often associates the personalized delivery of content with a more empowered individual user.Promises of individual empowerment attached to emerging expressions of personalized media are generally made in opposition to the broadcast media model of the mass market. The concluding section of this paper will examine the rhetoric of these promises in a discussion that considers such novel adaptations of content production and delivery as a market response to current media configurations. These adaptations, it will be argued, serve in large part to define differences to be ""commercially exploited."" [3] The nurturing of differentiated markets represents a potential challenge to some of the very characteristics that have traditionally been associated with the internet's social and political potential.",Proceedings of the 2011 IConference,2011,Research
Understanding book search behavior on the web,,Keep,"['Jin Kim', 'Henry Feild', 'Marc Cartright']",https://doi.org/10.1145/2396761.2396856,"With the increased availability of e-books and digitized book collections, more users are searching the web for information about books. There are many online digital libraries containing book, author and subject data, which are accessed via internal search services as well as external web sites, such as Google. Although this is a common yet complex information-seeking behavior involving multiple search systems with different characteristics, little is known about how users find information in this scenario.In this work, we analyze web-based book search behavior using three months of logs from the Open Library, a globally accessible digital library. Our study encompasses the user behavior on web search engines and the digital library, unlike previous work which focused on institution-level digital libraries. Among our findings are (1) query characteristics and session-level behaviors are drastically different between internal and external searchers; (2) the field usage is different based on the modes of interaction---keyword search, advanced search interface and faceted filtering; (3) users go through with more iterations of faceted filtering than query reformulation. To facilitate future research on book search, we also create a book search test collection based on the log data. We then perform an evaluation of several retrieval methods, finding that field-based retrieval models have advantages over document-based models.",Proceedings of the 21st ACM International Conference on Information and Knowledge Management,2012,Research
Lady Chatterley's Library: Books and Reading as Public Performance and Private Act,,Keep,"['Dana Mckay', 'Michael Twidale', 'George Buchanan']",https://doi.org/10.1145/3406522.3446032,"If reading was simply ingesting information, it would not matter what format that information arrived in-whether it be digital, poor quality newsprint, or the most beautiful hardback book. We do make choices about our reading, though, based on other factors than informational content. Some of these have been discussed in the literature, including convenience, preferences between paper and screen, and acquisition preferences. In this paper, we discuss books and reading as a performance. We examine the visual and cultural elements of books that influence the decisions we make about privacy or performance. We use these reflections to suggest a new form of information interaction, and make suggestions for a future program of research to support this new interaction.",Proceedings of the 2021 Conference on Human Information Interaction and Retrieval,2021,Research
Gender expression in a small world: social tagging of transgender-themed books,,Keep,['Melissa Adler'],missing,"Social tagging is evaluated as an information behavior in a small world, using Chatman, Burnett, and Besant's Theory of Normative Behavior. A survey was distributed to people who assign tags to transgender-themed books in LibraryThing, an social network site that allows members to catalog and tag their personal book collections. This study first establishes that there is an identifiable community of interest and then lays the groundwork for understanding the function of tagging in sharing information among a marginalized community for whom vocabulary and taxonomies are vital.",Proceedings of the 76th ASIS&amp;T Annual Meeting: Beyond the Cloud: Rethinking Information Boundaries,2013,Research
Dead Angles of Personalization: Integrating Curation Algorithms in the Fabric of Design,,Keep,['Nolwenn Maudet'],https://doi.org/10.1145/3322276.3322322,"The amount of information available on the web is too vast for individuals to be able to process it all. To cope with this issue, digital platforms started relying on algorithms to curate, filter and recommend content to their users. This problem has generally been envisioned from a technical perspective, as an optimization issue and has been mostly untouched by design considerations. Through 16 interviews with daily users of platforms, we analyze how curation algorithms influence their daily experience and the strategies they use to try to adapt them to their own needs. Based on these empirical findings, we propose a set of four speculative design alternatives to explore how we can integrate curation algorithms as part of the larger fabric of design on the web. By exploring interactions to counter the binary nature of curation algorithms, their uniqueness, their anti-historicity and their implicit data collection, we provide tools to bridge the current divide between curation algorithms and people.",Proceedings of the 2019 on Designing Interactive Systems Conference,2019,Research
