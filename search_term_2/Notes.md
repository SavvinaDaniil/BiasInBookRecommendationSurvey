# Search term 2

We look for incidental studies.
("book-crossing" OR "bookcrossing" OR "Librarything" OR "Amazon books" OR "Goodreads" OR "Goodbooks" )("bias") ("recommender") 
## Thursday, 26 September 2024

### Start
Let’s start with searches.

Google scholar

1610 results. Download issue, work on it!


## Thursday, 03 October 2024

### Processing
Now we start processing

Google scholar

Managed to download 899 results.

Scopus

60 results. Downloaded into table

Let’s process them.

Merged table. 959 TOTAL. 37 duplicates. 940 left.


Remove duplicates with the previous search term: 9 of them we already have, so 931 left.
### Screening
Screening starts! First screen only Title and Abstract.

What is relevant:  Bias means BIAS, not bias parameter in algorithms. Not theses. Studying these actual datasets. Not surveys.


OMG it's such a terrible thing to be doing. Ill try something different:
- Take the bias and debias paper, look for the book datasets in the paper they cite.
    - 227 papers, exported by scopus.
- Do the search term 2 only from 2022 onwards.



<span style="color:blue">Tomorrow</span>
-

## Wednesday, 09 October 2024
Continue with the initial screening...

No! New idea. Search only in top conferences and journals related to RS. List:
- [ ] WWW ACM
- [ ] WSDM ACM
- [ ] SIGIR ACM
- [ ] RecSys ACM 
- [ ] TOIS ACM


- [ ] UMAP ACM
- [ ] (ECIR)
- [ ] TORS ACM
- [ ] Frontiers in Big Data|Recommender Systems

Searched term from this file in ACM. 366 results. 259 research articles. 34 short papers. -> 293.\
Includes the following: RecSys, CIKM, WWW, SIGIR, KDD, WSDM, UMAP, CHI, etc.



## Tuesday, 29 October 2024
Screen these 293 papers. Idea discussed with Laura: Yes for papers that specifically are looking into bias, No for irrelevant, Keep for some incidental so we might include them!
Let's fucking do it.










47 relevant, 176 irrelevant (not about the topic at all, not about AI, only academic or research libraries, it was citation of a magazine article or something)

Now the next step is to remove more duplicates.

Done! We got rid of 16 and have 31.

Now we ll check retrievability. 7 not retrievable, so we check 24 for eligiblity.

Now we check eligibility.

## Wednesday, 25 September 2024

### Eligibility
We check for eligibility

We have 24 retrievable results.

15 not eligible, 9 eligible.

