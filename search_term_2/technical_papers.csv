Title,Notes,Link,Venue,Year,Authors,Type,Abstract
Flexibly manipulating popularity bias for tackling trade-offs in recommendation,"We show that conventional methods inevitably suffer from trade-offs regarding the popularity of items, whereas our method is flexible enough to meet the diverse needs of a platform and the users. amazonbooks",https://www.sciencedirect.com/science/article/pii/S0306457323003436,Information Processing & Management,2024,"['Hiroki Okamura', 'Keisuke Maeda', 'Ren Togo', 'Takahiro Ogawa', 'Miki Haseyama']",missing,"Latent factor models in collaborative filtering (CF) have been the state-of-the-art for over a decade and widely studied. Most models learn a representation of each user/item and calculate the inner product between them to perform a recommendation. Meanwhile, the inherent long-tailed data make the model training difficult and can cause popularity bias. Popularity bias mitigation has become one of the central themes of recent years. However, although the problems caused by popularity bias (e.g., the Matthew effect) should not be ignored, the popularity of an item implies its quality or trend. Popularity bias mitigation is rather undesirable for platforms and users who prefer popular items. In this study, we first focus on the inner product model and investigate the good property of the inner product for long-tailed data. The inner product is also employed in state-of-the-art CF models, and its effectiveness has been demonstrated empirically. We find that the inner product allows for modeling the long-tailed user–item interactions using the item vector magnitude while the representations remain identifiable. Based on this good property, we propose a method, DirectMag, that allows a platform and even the user to flexibly manipulate popularity bias. DirectMag determines the magnitude of the vectors directly after the training and can control the degree of popularity bias by adjusting only one parameter. In our experiments, we perform a detailed analysis that is not limited to the value of the average recommendation accuracy. We show that conventional methods inevitably suffer from trade-offs regarding the popularity of items, whereas our method is flexible enough to meet the diverse needs of a platform and the users."
Is diversity optimization always suitable? Toward a better understanding of diversity within recommendation approaches,This article aims at providing an in-depth analysis of the diversity performances of different recommender systems. librarything,https://www.sciencedirect.com/science/article/pii/S0306457321002053,Information Processing & Management,2021,"['Yu Du', 'Sylvie Ranwez', 'Nicolas Sutton-Charani', 'Vincent Ranwez']",missing,"The diversity of the item list suggested by recommender systems has been proven to impact user satisfaction significantly. Most of the existing diversity optimization approaches re-rank the list of candidate items during a post-processing step. However, the diversity level of the candidate list strongly depends on the recommender system used. Hence, applying the same post-processing diversification strategy may not be as effective for different recommendation approaches. Moreover, individual users’ diversity needs are usually ignored in the diversification post-processing. This article aims at providing an in-depth analysis of the diversity performances of different recommender systems. To the best of our knowledge, it is the first study to systematically compare diversity performances of the main types of recommendation models using benchmark datasets in different domains (movie, anime and book). Semantics related to items may be considered a key factor in measuring diversity within recommender systems. In this study, we leverage support from the knowledge engineering domain and take advantage of resources such as Linked Data and knowledge graphs, to assert the diversity of recommendations. We also propose a variant of the classic diversification post-processing objective that allows to take into account specific users’ diversity needs. We measure the adequacy between the diversity levels a recommender system suggests to its users and those of users’ profiles with the R2 coefficient of determination. Our study indicates that: (1) none of the tested recommender systems, even the most recent ones, provides items with levels of diversity that suit user profiles (R2<0.2); (2) the classic post-processing diversification approach may lead to over-diversification compared to users’ diversity needs and (3) the diversity adjustment that accounts for user profiles has more benefits (greater R2 and smaller accuracy loss). All the source code and datasets used in our study are available to ensure the reproductibility of the study."
Bias-Aware Hierarchical Clustering for detecting the discriminated groups of users in recommendation systems,Bias-Aware Hierarchical Clustering algorithm that identifies user clusters based on latent embeddings constructed by a black-box recommender to identify users whose needs are not met by the given recommendation method. bookcrossing,https://www.sciencedirect.com/science/article/pii/S0306457321000285,Information Processing & Management,2021,"['Joanna Misztal-Radecka', 'Bipin Indurkhya']",missing,"One challenge for the modern recommendation systems is the Tyranny of Majority — the generated recommendations are often optimized for the mainstream trends so that the minority preference groups remain discriminated. Moreover, most modern recommendation techniques are characterized as black-box systems. Given a lack of understanding of the dataset characteristics and insufficient diversity of represented individuals, such approaches inevitably lead to amplifying hidden data biases and existing disparities. In this research, we address this problem by proposing a novel approach to detecting and describing potentially discriminated user groups for a given recommendation algorithm. We propose a Bias-Aware Hierarchical Clustering algorithm that identifies user clusters based on latent embeddings constructed by a black-box recommender to identify users whose needs are not met by the given recommendation method. Next, a post-hoc explainer model is applied to reveal the most important descriptive features that characterize these user segments. Our method is model-agnostic and does not require any a priori information about existing disparities and sensitive attributes. An experimental evaluation on a synthetic dataset and two real-world datasets from different domains shows that, compared with other clustering methods and arbitrarily selected user groups, our method is capable of identifying underperforming segments for different recommendation algorithms, and detect more severe disparities."
Explaining recommender systems fairness and accuracy through the lens of data characteristics,This work provides a systematic study on the impact of broadly chosen data characteristics (DCs) of recommender systems. This is applied to the accuracy and fairness of several variations of CF recommendation models. bookcrossing,https://www.sciencedirect.com/science/article/pii/S0306457321001503,Information Processing & Management,2021,"['Yashar Deldjoo', 'Alejandro Bellogin', 'Tommaso {Di Noia}']",missing,"The impact of data characteristics on the performance of classical recommender systems has been recently investigated and produced fruitful results about the relationship they have with recommendation accuracy. This work provides a systematic study on the impact of broadly chosen data characteristics (DCs) of recommender systems. This is applied to the accuracy and fairness of several variations of CF recommendation models. We focus on a suite of DCs that capture properties about the structure of the user–item interaction matrix, the rating frequency, item properties, or the distribution of rating values. Experimental validation of the proposed system involved large-scale experiments by performing 23,400 recommendation simulations on three real-world datasets in the movie (ML-100K and ML-1M) and book domains (BookCrossing). The validation results show that the investigated DCs in some cases can have up to 90% of explanatory power – on several variations of classical CF algorithms –, while they can explain – in the best case – about 40% of fairness results (measured according to user gender and age sensitive attributes). Therefore, this work evidences that it is more difficult to explain variations in performance when dealing with fairness dimension than accuracy."
Provider fairness across continents in collaborative recommender systems,we focus on movie and book recommendation and enrich two datasets with the continent of production of an item. We use this data to characterize imbalances in the distribution of the user–item observations and regarding where items are produced (geographic imbalance). bookcrossing,https://www.sciencedirect.com/science/article/pii/S030645732100203X,Information Processing & Management,2022,"['Elizabeth Gómez', 'Ludovico Boratto', 'Maria Salamó']",missing,"When a recommender system suggests items to the end-users, it gives a certain exposure to the providers behind the recommended items. Indeed, the system offers a possibility to the items of those providers of being reached and consumed by the end-users. Hence, according to how recommendation lists are shaped, the experience of under-recommended providers in online platforms can be affected. To study this phenomenon, we focus on movie and book recommendation and enrich two datasets with the continent of production of an item. We use this data to characterize imbalances in the distribution of the user–item observations and regarding where items are produced (geographic imbalance). To assess if recommender systems generate a disparate impact and (dis)advantage a group, we divide items into groups, based on their continent of production, and characterize how represented is each group in the data. Then, we run state-of-the-art recommender systems and measure the visibility and exposure given to each group. We observe disparities that favor the most represented groups. We overcome these phenomena by introducing equity with a re-ranking approach that regulates the share of recommendations given to the items produced in a continent (visibility) and the positions in which items are ranked in the recommendation list (exposure), with a negligible loss in effectiveness, thus controlling fairness of providers coming from different continents. A comparison with the state of the art shows that our approach can provide more equity for providers, both in terms of visibility and of exposure."
Spiral of Silence and Its Application in Recommender Systems,"we develop four variants to model missing ratings. They mimic different components of the spiral of silence based on the spiral process with global opinion climate, local opinion climate, hardcore users, relationships between hardcore users and items, respectively. amazon books",10.1109/TKDE.2020.3013973,IEEE Transactions on Knowledge and Data Engineering,2022,"['Chen Lin', 'Dugang Liu', 'Hanghang Tong', 'Yanghua Xiao']",missing,"It is crucial to model missing ratings in recommender systems since user preferences learnt from only observed ratings are biased. One possible explanation for missing ratings is motivated by the spiral of silence theory. When the majority opinion is formed, a spiral process is triggered where users are more and more likely to show their ratings if they perceive that they are supported by the opinion climate. In this paper we first verify the existence of the spiral process in recommender systems by using a variety of different real-life datasets. We then study the characteristics of two key factors in the spiral process: opinion climate and the hardcore users who will give ratings even when they are minority opinion holders. Based on our empirical findings, we develop four variants to model missing ratings. They mimic different components of the spiral of silence based on the spiral process with global opinion climate, local opinion climate, hardcore users, relationships between hardcore users and items, respectively. We experimentally show that, the presented variants all outperform state-of-the-art recommendation models with missing rating components."
Exploring author gender in book rating and recommendation,"Using publicly available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. bookcrossing etc",http://link.springer.com/article/10.1007/s11257-020-09284-2,User Modeling and User-Adapted Interaction,2021,Michael D. EkstrandDaniel Kluver,Article,missing
What recommenders recommend: an analysis of recommendation biases and possible countermeasures,we show that popular recommendation techniques—despite often being similar when compared with the help of accuracy measures—can be quite different with respect to which items they recommend. BookCrossing,http://link.springer.com/article/10.1007/s11257-015-9165-3,User Modeling and User-Adapted Interaction,2015,Dietmar JannachLukas LercheIman KamehkhoshMichael Jugovac,Article,missing
A bias detection tree approach for detecting disparities in a recommendation model’s errors,a model-agnostic technique to automatically detect the combinations of user and item attributes correlated with unequal treatment by the recommendation model. Bookcrossing,http://link.springer.com/article/10.1007/s11257-022-09334-x,User Modeling and User-Adapted Interaction,2023,Joanna Misztal-RadeckaBipin Indurkhya,Article,missing
Exploring potential biases towards blockbuster items in ranking-based recommendations,"we attempt to consider items that are both popular and highly-liked, which we refer to as blockbuster items, and to investigate whether the recommendation algorithms impose a considerable bias in favor of the blockbuster items in their ranking-based recommendations. Goodbooks",http://link.springer.com/article/10.1007/s10618-022-00860-1,Data Mining and Knowledge Discovery,2022,Emre Yalcin,Article,missing
The Unfairness of Popularity Bias in Book Recommendation,"most state-of-the-art recommendation algorithms suffer from popularity bias in the book domain, and fail to meet users’ expectations with Niche and Diverse tastes despite having a larger profile size. bookcrossing",http://link.springer.com/chapter/10.1007/978-3-031-09316-6_7,Advances in Bias and Fairness in Information Retrieval,2022,Mohammadmehdi NaghiaeiHossein A. RahmaniMahdi Dehghan,Chapter,missing
Popularity Bias in Collaborative Filtering-Based Multimedia Recommender Systems,we evaluate four collaborative filtering-based algorithms with respect to popularity bias on the item and the user level. bookcrossing,http://link.springer.com/chapter/10.1007/978-3-031-09316-6_1,Advances in Bias and Fairness in Information Retrieval,2022,Dominik KowaldEmanuel Lacic,Chapter,missing
Utilizing Implicit Feedback for User Mainstreaminess Evaluation and Bias Detection in Recommender Systems,we complete the data preprocessing steps missing in the original paper and reproduce the evaluation experiments. Bookcrossing,http://link.springer.com/chapter/10.1007/978-3-031-37249-0_4,Advances in Bias and Fairness in Information Retrieval,2023,Kuanyi ZhangMin XieYi ZhangHaixian Zhang,Chapter,missing
An Offer You Cannot Refuse? Trends in the Coercive Impact of Amazon Book Recommendations,This paper aims to investigate whether Amazon’s recommender system has made it more difficult to change preferences over time. Amazon books,http://link.springer.com/chapter/10.1007/978-3-031-71975-2_1,Advances in Bias and Fairness in Information Retrieval,2025,Jonathan H. Rystrøm,Chapter,missing
Privacy-Preserving Fair Item Ranking,This work is the first to advance research at the conjunction of producer (item) fairness and consumer (user) privacy in rankings by exploring the incorporation of privacy-preserving techniques. bookcrossing,http://link.springer.com/chapter/10.1007/978-3-031-28238-6_13,Advances in Information Retrieval,2023,Jia Ao SunSikha PentyalaMartine De CockGolnoosh Farnadi,Chapter,missing
Towards Optimizing Ranking in Grid-Layout for Provider-Side Fairness,"Providing fair exposure to providers in such layouts is not well-studied. We seek to fill this gap by providing a grid-aware re-ranking algorithm to optimize layouts for provider-side fairness by adapting existing re-ranking techniques to grid-aware browsing models, and an analysis of the effect of grid-specific factors such as device size on the resulting fairness optimization. GoodReads+PIReT Book Data Tools",http://link.springer.com/chapter/10.1007/978-3-031-56069-9_7,Advances in Information Retrieval,2024,Amifa RajMichael D. Ekstrand,Chapter,missing
On the instability of embeddings for recommender systems: the case of matrix factorization,"by simply changing the initial values assigned to the latent factors, the same MF method generates very different embeddings of items and users, and we highlight that this effect is stronger for less popular items.",https://doi.org/10.1145/3412841.3442011,Proceedings of the 36th Annual ACM Symposium on Applied Computing,2021,"['Giovanni Gabbolini', ""Edoardo D'Amico"", 'Cesare Bernardis', 'Paolo Cremonesi']",Research,"Most state-of-the-art top-N collaborative recommender systems work by learning embeddings to jointly represent users and items. Learned embeddings are considered to be effective to solve a variety of tasks. Among others, providing and explaining recommendations. In this paper we question the reliability of the embeddings learned by Matrix Factorization (MF). We empirically demonstrate that, by simply changing the initial values assigned to the latent factors, the same MF method generates very different embeddings of items and users, and we highlight that this effect is stronger for less popular items. To overcome these drawbacks, we present a generalization of MF, called Nearest Neighbors Matrix Factorization (NNMF). The new method propagates the information about items and users to their neighbors, speeding up the training procedure and extending the amount of information that supports recommendations and representations. We describe the NNMF variants of three common MF approaches, and with extensive experiments on five different datasets we show that they strongly mitigate the instability issues of the original MF versions and they improve the accuracy of recommendations on the long-tail."
"Understanding Assimilation-contrast Effects in Online Rating Systems: Modelling, Debiasing, and Applications",missing,https://doi.org/10.1145/3362651,missing,2019,"['Xiaoying Zhang', 'Hong Xie', 'Junzhou Zhao', 'John Lui']",Research,"“Unbiasedness,” which is an important property to ensure that users’ ratings indeed reflect their true evaluations of products, is vital both in shaping consumer purchase decisions and providing reliable recommendations in online rating systems. Recent experimental studies showed that distortions from historical ratings would ruin the unbiasedness of subsequent ratings. How to “discover” historical distortions in each single rating (or at the micro-level), and perform the “debiasing operations” are our main objective. Using 42M real customer ratings, we first show that users either “assimilate” or “contrast” to historical ratings under different scenarios, which can be further explained by a well-known psychological argument: the “Assimilate-Contrast” theory. This motivates us to propose the Historical Influence Aware Latent Factor Model (HIALF), the “first” model for real rating systems to capture and mitigate historical distortions in each single rating. HIALF allows us to study the influence patterns of historical ratings from a modelling perspective, which perfectly matches the assimilation and contrast effects observed in experiments. Moreover, HIALF achieves significant improvements in predicting subsequent ratings and characterizing relationships in ratings. It also contributes to better recommendations, wiser consumer purchase decisions, and deeper understanding of historical distortions in both honest rating and misbehaving rating settings."
Promoting Two-sided Fairness with Adaptive Weights for Providers and Customers in Recommendation,"the recommender is trained with two-sided fairness-aware
weights to boost the utility of niche providers and inactive customers in a unified way. Book-crossing",https://doi.org/10.1145/3640457.3688169,Proceedings of the 18th ACM Conference on Recommender Systems,2024,"['Lanling Xu', 'Zihan Lin', 'Jinpeng Wang', 'Sheng Chen', 'Wayne Zhao', 'Ji-Rong Wen']",Short,"At present, most recommender systems involve two stakeholders, providers and customers. Apart from maximizing the recommendation accuracy, the fairness issue for both sides should also be considered. Most of previous studies try to improve two-sided fairness with post-processing algorithms or fairness-aware loss constraints, which are highly dependent on the heuristic adjustments without respect to the optimization goal of accuracy. In contrast, we propose a novel training framework, adaptive weighting towards two-sided fairness-aware recommendation&nbsp;(named Ada2Fair), which lies in the extension of the accuracy-focused objective to a controllable preference learning loss over the interaction data. Specifically, we adjust the optimization scale of an interaction sample with an adaptive weight generator, and estimate the two-sided fairness-aware weights within model training. During the training process, the recommender is trained with two-sided fairness-aware weights to boost the utility of niche providers and inactive customers in a unified way. Extensive experiments on three public datasets verify the effectiveness of&nbsp;Ada2Fair, which can achieve Pareto efficiency in two-sided fairness-aware recommendation."
Reproducing Popularity Bias in Recommendation: The Effect of Evaluation Strategies,A study that reproduces other work on evaluating popularity bias -- one of them is a book dataset.,https://doi.org/10.1145/3637066,missing,2024,"['Savvina Daniil', 'Mirjam Cuper', 'Cynthia Liem', 'Jacco Ossenbruggen', 'Laura Hollink']",Research,"The extent to which popularity bias is propagated by media recommender systems is a current topic within the community, as is the uneven propagation among users with varying interests for niche items. Recent work focused on exactly this topic, with movies being the domain of interest. Later on, two different research teams reproduced the methodology in the domains of music and books, respectively. The results across the different domains diverge. In this paper, we reproduce the three studies and identify four aspects that are relevant in investigating the differences in results: data, algorithms, division of users in groups and evaluation strategy. We run a set of experiments in which we measure general popularity bias propagation and unfair treatment of certain users with various combinations of these aspects. We conclude that all aspects account to some degree for the divergence in results, and should be carefully considered in future studies. Further, we find that the divergence in findings can be in large part attributed to the choice of evaluation strategy."
Toward Bias-Agnostic Recommender Systems: A Universal Generative Framework,"universal Generative framework for Bias
Disentanglement constantly generating calibration perturbations for the intermediate representations during training to keep them from being affected by the bias. Book-crossing",https://doi.org/10.1145/3655617,missing,2024,"['Zhidan Wang', 'Lixin Zou', 'Chenliang Li', 'Shuaiqiang Wang', 'Xu Chen', 'Dawei Yin', 'Weidong Liu']",Research,"User behavior data, such as ratings and clicks, has been widely used to build personalizing models for recommender systems. However, many unflattering factors&nbsp;(e.g., popularity, ranking position, users’ selection) significantly affect the performance of the learned recommendation model. Most existing work on unbiased recommendation addressed these biases from sample granularity&nbsp;(e.g., sample reweighting, data augmentation) or from the perspective of representation learning&nbsp;(e.g., bias-modeling). However, these methods are usually designed for a specific bias, lacking the universal capability to handle complex situations where multiple biases co-exist. Besides, rare work frees itself from laborious and sophisticated debiasing configurations&nbsp;(e.g., propensity scores, imputed values, or user behavior-generating process).Towards this research gap, in this article, we propose a universal Generative framework for Bias Disentanglement termed as GBD, constantly generating calibration perturbations for the intermediate representations during training to keep them from being affected by the bias. Specifically, a bias-identifier that tries to retrieve the bias-related information from the representations is first introduced. Subsequently, the calibration perturbations are generated to significantly deteriorate the bias-identifier’s performance, making the bias gradually disentangled from the calibrated representations. Therefore, without relying on notorious debiasing configurations, a bias-agnostic model is obtained under the guidance of the bias identifier. We further present its universality by subsuming the representative biases and their mixture under the proposed framework. Finally, extensive experiments on the real-world, synthetic, and semi-synthetic datasets have demonstrated the superiority of the proposed approach against a wide range of recommendation debiasing methods. The code is available at ."
What We Evaluate When We Evaluate Recommender Systems: Understanding Recommender Systems’ Performance using Item Response Theory,"we use item response theory (IRT), a family
of latent variable models used in psychometric assessment, to gain
a comprehensive understanding of offline evaluation. Amazon books",https://doi.org/10.1145/3604915.3608809,Proceedings of the 17th ACM Conference on Recommender Systems,2023,"['Yang Liu', 'Alan Medlar', 'Dorota Glowacka']",Research,"Current practices in offline evaluation use rank-based metrics to measure the quality of top-n recommendation lists. This approach has practical benefits as it centres assessment on the output of the recommender system and, therefore, measures performance from the perspective of end-users. However, this methodology neglects how recommender systems more broadly model user preferences, which is not captured by only considering the top-n recommendations. In this article, we use item response theory (IRT), a family of latent variable models used in psychometric assessment, to gain a comprehensive understanding of offline evaluation. We use IRT to jointly estimate the latent abilities of 51 recommendation algorithms and the characteristics of 3 commonly used benchmark data sets. For all data sets, the latent abilities estimated by IRT suggest that higher scores from traditional rank-based metrics do not reflect improvements in modeling user preferences. Furthermore, we show that the top-n recommendations with the most discriminatory power are biased towards lower difficulty items, leaving much room for improvement. Lastly, we highlight the role of popularity in evaluation by investigating how user engagement and item popularity influence recommendation difficulty."
A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems,"CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates
fairness constraints from both the consumer and producer side in a joint objective framework. Book-crossing",https://doi.org/10.1145/3651167,missing,2024,"['Hossein Rahmani', 'Mohammadmehdi Naghiaei', 'Yashar Deldjoo']",Research,"In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these ML systems that aid users in making decisions. The majority of past literature research on recommender systems fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this article, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their&nbsp;activity level and&nbsp;main-streamness, while producer groups are defined according to their popularity level. For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases. Our results on different group segmentation also indicate that the amount of improvement can vary and is dependent on group segmentation, indicating that the amount of bias produced and how much the algorithm can improve it depend on the protected group definition, a factor that, to our knowledge, has not been examined in great depth in previous studies but rather is highlighted by the results discovered in this study."
Biased User History Synthesis for Personalized Long-Tail Item Recommendation,"This long-tail item problem can exacerbate model bias, and reinforce poor recommendation of tail items. In this paper, we propose biased user history synthesis, to not only address this problem but also achieve better personalization in recommendation systems. Book-crossing",https://doi.org/10.1145/3640457.3688141,Proceedings of the 18th ACM Conference on Recommender Systems,2024,"['Keshav Balasubramanian', 'Abdulla Alshabanah', 'Elan Markowitz', 'Greg Ver Steeg', 'Murali Annavaram']",Research,"Recommendation systems connect users to items and create value chains in the internet economy. Recommendation systems learn from past user-item interaction histories. As such, items that have short interaction histories, either because they are new or not popular, have been shown to be disproportionately under-recommended. This long-tail item problem can exacerbate model bias, and reinforce poor recommendation of tail items. In this paper, we propose biased user history synthesis, to not only address this problem but also achieve better personalization in recommendation systems. As a result, we concurrently improve tail and head item recommendation performance. Our approach is built on a tail item biased User Interaction History (UIH) sampling strategy and a synthesis model that produces an augmented user representation from the sampled user history. We provide a theoretical justification for our approach using information theory and demonstrate through extensive experimentation, that our model outperforms state-of-the-art baselines on tail, head, and overall recommendation. The source code is available at https://github.com/lkp411/BiasedUserHistorySynthesis."
Countering Popularity Bias by Regularizing Score Differences,"a novel method to reduce the model bias while maintaining
accuracy by directly regularizing the recommendation scores to be
equal across items a user preferred. goodreads",https://doi.org/10.1145/3523227.3546757,Proceedings of the 16th ACM Conference on Recommender Systems,2022,"['Wondo Rhee', 'Sung Cho', 'Bongwon Suh']",Research,"Recommendation system often suffers from popularity bias. Often the training data inherently exhibits long-tail distribution in item popularity (data bias). Moreover, the recommendation systems could give unfairly higher recommendation scores to popular items even among items a user equally liked, resulting in over-recommendation of popular items (model bias). In this study we propose a novel method to reduce the model bias while maintaining accuracy by directly regularizing the recommendation scores to be equal across items a user preferred. Akin to contrastive learning, we extend the widely used pairwise loss (BPR loss) which maximizes the score differences between preferred and unpreferred items, with a regularization term that minimizes the score differences within preferred and unpreferred items, respectively, thereby achieving both high debias and high accuracy performance with no additional training. To test the effectiveness of the proposed method, we design an experiment using a synthetic dataset which induces model bias with baseline training; we showed applying the proposed method resulted in drastic reduction of model bias while maintaining accuracy. Comprehensive comparison with earlier debias methods showed the proposed method had advantages in terms of computational validity and efficiency. Further empirical experiments utilizing four benchmark datasets and four recommendation models indicated the proposed method showed general improvements over performances of earlier debias methods. We hope that our method could help users enjoy diverse recommendations promoting serendipitous findings. Code available at https://github.com/stillpsy/popbias."
Comprehensive Fair Meta-learned Recommender System,"comprehensive fair meta-learning framework for ensuring the fairness of meta-learned
recommendation models. book-crossing.",https://doi.org/10.1145/3534678.3539269,Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2022,"['Tianxin Wei', 'Jingrui He']",Research,"In recommender systems, one common challenge is the cold-start problem, where interactions are very limited for fresh users in the systems. To address this challenge, recently, many works introduce the meta-optimization idea into the recommendation scenarios, i.e. learning to learn the user preference by only a few past interaction items. The core idea is to learn global shared meta-initialization parameters for all users and rapidly adapt them into local parameters for each user respectively. They aim at deriving general knowledge across preference learning of various users, so as to rapidly adapt to the future new user with the learned prior and a small amount of training data. However, previous works have shown that recommender systems are generally vulnerable to bias and unfairness. Despite the success of meta-learning at improving the recommendation performance with cold-start, the fairness issues are largely overlooked.In this paper, we propose a comprehensive fair meta-learning framework, named CLOVER, for ensuring the fairness of meta-learned recommendation models. We systematically study three kinds of fairness - individual fairness, counterfactual fairness, and group fairness in the recommender systems, and propose to satisfy all three kinds via a multi-task adversarial learning scheme. Our framework offers a generic training paradigm that is applicable to different meta-learned recommender systems. We demonstrate the effectiveness of CLOVER on the representative meta-learned user preference estimator on three real-world data sets. Empirical results show that CLOVER achieves comprehensive fairness without deteriorating the overall cold-start recommendation performance."
Experiments on Generalizability of User-Oriented Fairness in Recommender Systems,"we re-produce a user-oriented fairness study and provide extensive experiments to analyze the dependency of their proposed method on various fairness and recommendation aspects, including
the recommendation domain, nature of the base ranking model, and user grouping method. book crossing",https://doi.org/10.1145/3477495.3531718,Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,2022,"['Hossein Rahmani', 'Mohammadmehdi Naghiaei', 'Mahdi Dehghan', 'Mohammad Aliannejadi']",Research,"Recent work in recommender systems mainly focuses on fairness in recommendations as an important aspect of measuring recommendations quality. A fairness-aware recommender system aims to treat different user groups similarly. Relevant work on user-oriented fairness highlights the discriminant behavior of fairness-unaware recommendation algorithms towards a certain user group, defined based on users' activity level. Typical solutions include proposing a user-centered fairness re-ranking framework applied on top of a base ranking model to mitigate its unfair behavior towards a certain user group i.e., disadvantaged group. In this paper, we re-produce a user-oriented fairness study and provide extensive experiments to analyze the dependency of their proposed method on various fairness and recommendation aspects, including the recommendation domain, nature of the base ranking model, and user grouping method. Moreover, we evaluate the final recommendations provided by the re-ranking framework from both user- (e.g., NDCG, user-fairness) and item-side (e.g., novelty, item-fairness) metrics. We discover interesting trends and trade-offs between the model's performance in terms of different evaluation metrics. For instance, we see that the definition of the advantaged/disadvantaged user groups plays a crucial role in the effectiveness of the fairness algorithm and how it improves the performance of specific base ranking models. Finally, we highlight some important open challenges and future directions in this field. We release the data, evaluation pipeline, and the trained models publicly on https://github.com/rahmanidashti/FairRecSys."
Measuring Commonality in Recommendation of Cultural Content to Strengthen Cultural Citizenship,we introduce commonality as a new measure of recommender systems that reflects the degree to which recommendations familiarize a given user population with specified categories of cultural content.. Librarything.,https://doi.org/10.1145/3643138,missing,2024,"['Andres Ferraro', 'Gustavo Ferreira', 'Fernando Diaz', 'Georgina Born']",Research,"Recommender systems have become the dominant means of curating cultural content, significantly influencing the nature of individual cultural experience. While the majority of academic and industrial research on recommender systems optimizes for personalized user experience, this paradigm does not capture the ways that recommender systems impact cultural experience in the aggregate, across populations of users. Although existing novelty, diversity, and fairness studies probe how recommender systems relate to the broader social role of cultural content, they do not adequately center culture as a core concept and challenge. In this work, we introduce commonality as a new measure of recommender systems that reflects the degree to which recommendations familiarize a given user population with specified categories of cultural content. Our proposed commonality metric responds to a set of arguments developed through an interdisciplinary dialogue between researchers in computer science and the social sciences and humanities. With reference to principles underpinning public service media (PSM) systems in democratic societies, we identify universality of address and content diversity in the service of strengthening cultural citizenship as particularly relevant goals for recommender systems delivering cultural content. We develop commonality as a measure of recommender system alignment with the promotion of a shared cultural experience of, and exposure to, diverse cultural content across a population of users. Moreover, we advocate for the involvement of human editors accountable to a larger value community as a fundamental part of defining categories in the service of cultural citizenship. We empirically compare the performance of recommendation algorithms using commonality with existing utility, diversity, novelty, and fairness metrics using three different domains. Our results demonstrate that commonality captures a property of system behavior complementary to existing metrics and suggests the need for alternative, non-personalized interventions in recommender systems oriented to strengthening cultural citizenship across populations of users. Moreover, commonality demonstrates both consistent results under different editorial policies and robustness to missing labels and users. Alongside existing fairness and diversity metrics, commonality contributes to a growing body of scholarship developing “public good” rationales for digital media and machine learning systems."
Fair Projections as a Means Towards Balanced Recommendations,"Hm. I need to read better. PolBooks, Amazon books",https://doi.org/10.1145/3664929,missing,2024,"['Aris Anagnostopoulos', 'Luca Becchetti', 'Matteo B\\""{o}hm', 'Adriano Fazzone', 'Stefano Leonardi', 'Cristina Menghini', 'Chris Schwiegelshohn']",Research,"The goal of recommender systems is to provide to users suggestions that match their interests, with the eventual goal of increasing their satisfaction, as measured by the number of transactions (clicks, purchases, etc.). Often, this leads to providing recommendations that are of a particular type. For some contexts (e.g., browsing videos for information) this may be undesirable, as it may enforce the creation of filter bubbles. This is because of the existence of underlying bias in the input data of prior user actions.Reducing hidden bias in the data and ensuring fairness in algorithmic data analysis has recently received significant attention. In this paper, we consider both the densest subgraph and the (k) -clustering problem, two primitives that are being used by some recommender systems. We are given a coloring on the nodes, respectively the points, and aim to compute a fair solution (S) , consisting of a subgraph or a clustering, such that none of the colors is disparately impacted by the solution.Unfortunately, introducing fair solutions typically makes these problems substantially more difficult. Unlike the unconstrained densest subgraph problem, which is solvable in polynomial time, the fair densest subgraph problem is NP-hard even to approximate. For (k) -clustering, the fairness constraints make the problem very similar to capacitated clustering, which is a notoriously hard problem to even approximate.Despite such negative premises, we are able to provide positive results in important use cases. In particular, we are able to prove that a suitable spectral embedding allows recovery of an almost optimal, fair, dense subgraph hidden in the input data, whenever one is present, a result that is further supported by experimental evidence.We also show a polynomial-time, (2) -approximation algorithm to the problem of fair densest subgraph, assuming that there exist only two colors and both colors occur equally often in the graph. This result turns out to be optimal assuming the small set expansion hypothesis. For fair (k) -clustering, we show that we can recover high quality fair clusterings effectively and efficiently. For the special case of (k) -median and (k) -center, we offer additional, fast and simple approximation algorithms as well as new hardness results.The above theoretical findings drive the design of heuristics, which we experimentally evaluate on a scenario based on real data, in which our aim is to strike a good balance between diversity and highly correlated items from Amazon co-purchasing graphs and facebook contacts. We additionally evaluated our algorithmic solutions for the fair (k) -median problem through experiments on various real-world datasets."
Not All Embeddings are Created Equal: Towards Robust Cross-domain Recommendation via Contrastive Learning,"such bias in creating embeddings reveals the fact that “not all embeddings are
created equal” in CDR, which serves as the primary motivation of this study. amazon books",https://doi.org/10.1145/3589334.3645357,Proceedings of the ACM Web Conference 2024,2024,"['Wenhao Yang', 'Yingchun Jian', 'Yibo Wang', 'Shiyin Lu', 'Lei Shen', 'Bing Wang', 'Haihong Tang', 'Lijun Zhang']",Research,"Cross-domain recommendation (CDR) aims to leverage the rich information from the source domain to enhance recommendation performance in the target domain. However, the data imbalance problem inherent across different domains compromises the effectiveness of CDR approaches, posing a significant challenge to CDR. Most current CDR methodologies focus on creating better user embeddings for the target domain, yet usually neglect the inconsistency in user activities due to data imbalance. As a result, the process of creating user embeddings tends to prioritize users with more frequent interactions and leave less active users underserved, leading these CDR methods to struggle in making accurate recommendations for those with fewer interactions. Such bias in creating embeddings reveals the fact that ''not all embeddings are created equal'' in CDR, which serves as the primary motivation of this study. Inspired by the recent development of contrastive learning, this paper proposes User-aware Contrastive Learning for Robust cross-domain recommendation (UCLR), enhancing the robustness of cross-domain recommendation. Specifically, our proposed method consists of two sub-modules: (i) pretrained global embedding, where the global user embeddings are pretrained across all the domains; (ii) contrastive dual-stream collaborative autoencoder, where more equal user embeddings are generated by optimizing contrastive loss with individualized temperatures. To further improve the performance of our method in each domain, we finetune the whole framework of UCLR based on Low-Rank Adaptation (LoRA). Theoretically, our method is equipped with a provable convergence guarantee during the contrastive learning stage. Furthermore, we also conduct comprehensive experiments on real-world datasets to validate the effectiveness of our proposed method."
CPFair: Personalized Consumer and Producer Fairness Re-ranking for Recommender Systems,Duplicate,https://doi.org/10.1145/3477495.3531959,Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,2022,"['Mohammadmehdi Naghiaei', 'Hossein Rahmani', 'Yashar Deldjoo']",Research,"Recently, there has been a rising awareness that when machine learning (ML) algorithms are used to automate choices, they may treat/affect individuals unfairly, with legal, ethical, or economic consequences. Recommender systems are prominent examples of such ML systems that assist users in making high-stakes judgments. A common trend in the previous literature research on fairness in recommender systems is that the majority of works treat user and item fairness concerns separately, ignoring the fact that recommender systems operate in a two-sided marketplace. In this work, we present an optimization-based re-ranking approach that seamlessly integrates fairness constraints from both the consumer and producer-side in a joint objective framework. We demonstrate through large-scale experiments on 8 datasets that our proposed method is capable of improving both consumer and producer fairness without reducing overall recommendation quality, demonstrating the role algorithms may play in minimizing data biases."
Improving the Estimation of Tail Ratings in Recommender System with Multi-Latent Representations,"We propose to improve the estimation of tail ratings by extending traditional single latent representations (e.g., an item is represented by a single latent vector) with new multi-latent representations for better modeling these tail ratings. Amazon books",https://doi.org/10.1145/3336191.3371810,Proceedings of the 13th International Conference on Web Search and Data Mining,2020,"['Xing Zhao', 'Ziwei Zhu', 'Yin Zhang', 'James Caverlee']",Research,"The importance of the distribution of ratings on recommender systems (RS) is well-recognized. And yet, recommendation approaches based on latent factor models and recently introduced neural variants (e.g., NCF) optimize for the head of these distributions, potentially leading to large estimation errors for tail ratings. These errors in tail ratings that are far from the mean predicted rating fall out of a uni-modal assumption underlying these popular models, as we show in this paper. We propose to improve the estimation of tail ratings by extending traditional single latent representations (e.g., an item is represented by a single latent vector) with new multi-latent representations for better modeling these tail ratings. We show how to incorporate these multi-latent representations in an end-to-end neural prediction model that is designed to better reflect the underlying ratings distributions of items. Through experiments over six datasets, we find the proposed model leads to a significant improvement in RMSE versus a suite of benchmark methods. We also find that the predictions for the most polarized items are improved by more than 15%."
Distributional Fairness-aware Recommendation,"we define a novel type of fairness, where we require that the performance distributions across different user groups should be similar. Book crossing",https://doi.org/10.1145/3652854,missing,2024,"['Hao Yang', 'Xian Wu', 'Zhaopeng Qiu', 'Yefeng Zheng', 'Xu Chen']",Research,"Fairness has been gradually recognized as a significant problem in the recommendation domain. Previous models usually achieve fairness by reducing the average performance gap between different user groups. However, the average performance may not sufficiently represent all the characteristics of the performances in a user group. Thus, equivalent average performance may not mean the recommender model is fair, for example, the variance of the performances can be different. To alleviate this problem, in this article, we define a novel type of fairness, where we require that the performance distributions across different user groups should be similar. We prove that with the same performance distribution, the numerical characteristics of the group performance, including the expectation, variance, and any higher-order moment, are also the same. To achieve distributional fairness, we propose a generative and adversarial training framework. Specifically, we regard the recommender model as the generator to compute the performance for each user in different groups, and then we deploy a discriminator to judge which group the performance is drawn from. By iteratively optimizing the generator and the discriminator, we can theoretically prove that the optimal generator (the recommender model) can indeed lead to the equivalent performance distributions. To smooth the adversarial training process, we propose a novel dual curriculum learning strategy for optimal scheduling of training samples. Additionally, we tailor our framework to better suit top-N recommendation tasks by incorporating softened ranking metrics as measures of performance discrepancies. We conduct extensive experiments based on real-world datasets to demonstrate the effectiveness of our model."
Optimizing Neighborhoods for Fair Top-N Recommendation,We address demographic bias in neighborhood-learning models for collaborative filtering recommendations. Goodreads,https://doi.org/10.1145/3627043.3659539,"Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization",2024,"['Stavroula Eleftherakis', 'Georgia Koutrika', 'Sihem Amer-Yahia']",Research,"We address demographic bias in neighborhood-learning models for collaborative filtering recommendations. Despite their superior ranking performance, these methods can learn neighborhoods that inadvertently foster discriminatory patterns. Little work exists in this area, highlighting an important research gap. A notable yet solitary effort, Balanced Neighborhood Sparse LInear Method (BNSLIM) aims at balancing neighborhood influence across different demographic groups. Yet, BNSLIM is hampered by computational inefficiency, and its rigid balancing approach often impacts accuracy. In that vein, we introduce two novel algorithms. The first, an enhancement of BNSLIM, incorporates the Alternating Direction Method of Multipliers (ADMM) to optimize all similarities concurrently, greatly reducing training time. The second, Fairly Sparse Linear Regression (FSLR), induces controlled sparsity in neighborhoods to reveal correlations among different demographic groups, achieving comparable efficiency while being more accurate. Their performance is evaluated using standard exposure metrics alongside a new metric for user coverage disparities. Our experiments cover various applications, including a novel exploration of bias in course recommendations by teachers’ country development status. Our results show the effectiveness of our algorithms in imposing fairness compared to BNSLIM and other well-known fairness approaches."
Spiral of Silence in Recommender Systems,In this paper we present one possible explanation of the missing not at random phenomenon. amazon books,https://doi.org/10.1145/3289600.3291003,Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining,2019,"['Dugang Liu', 'Chen Lin', 'Zhilin Zhang', 'Yanghua Xiao', 'Hanghang Tong']",Research,"It has been established that, ratings are missing not at random in recommender systems. However, little research has been done to reveal how the ratings are missing. In this paper we present one possible explanation of the missing not at random phenomenon. We verify that, using a variety of different real-life datasets, there is a spiral process for a silent minority in recommender systems where (1) people whose opinions fall into the minority are less likely to give ratings than majority opinion holders; (2) as the majority opinion becomes more dominant, the rating possibility of a majority opinion holder is intensifying but the rating possibility of a minority opinion holder is shrinking; (3) only hardcore users remain to rate for minority opinions when the spiral achieves its steady state. Our empirical findings are beneficial for future recommendation models. To demonstrate the impact of our empirical findings, we present a probabilistic model that mimics the generation process of spiral of silence. We experimentally show that, the presented model offers more accurate recommendations, compared with state-of-the-art recommendation models."
Dual-Side Adversarial Learning Based Fair Recommendation for Sensitive Attribute Filtering,"we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. book crossing",https://doi.org/10.1145/3648683,missing,2024,"['Shenghao Liu', 'Yu Zhang', 'Lingzhi Yi', 'Xianjun Deng', 'Laurence Yang', 'Bang Wang']",Research,"With the development of recommendation algorithms, researchers are paying increasing attention to fairness issues such as user discrimination in recommendations. To address these issues, existing works often filter users’ sensitive information that may cause discrimination during the process of learning user representations. However, these approaches overlook the latent relationship between items’ content attributes and users’ sensitive information. In this article, we propose DALFRec, a fairness-aware recommendation algorithm based on user-side and item-side adversarial learning to mitigate the effects of sensitive information on both sides of the recommendation process. First, we conduct a statistical analysis to demonstrate the latent relationship between items’ information and users’ sensitive attributes. Then, we design a dual-side adversarial learning network that simultaneously filters out users’ sensitive information on the user and item side. Additionally, we propose a new evaluation strategy that leverages the latent relationship between items’ content attributes and users’ sensitive attributes to better assess the algorithm’s ability to reduce discrimination. Our experiments on three real datasets demonstrate the superiority of our proposed algorithm over state-of-the-art methods."
Meta Graph Learning for Long-tail Recommendation,novel idea to learn relations among items as an auxiliary graph to enhance the graph-based representation learning and make recommendations collectively in a coupled framework. Bookcrossing,https://doi.org/10.1145/3580305.3599428,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2023,"['Chunyu Wei', 'Jian Liang', 'Di Liu', 'Zehui Dai', 'Mang Li', 'Fei Wang']",Research,"Highly skewed long-tail item distribution commonly hurts model performance on tail items in recommendation systems, especially for graph-based recommendation models. We propose a novel idea to learn relations among items as an auxiliary graph to enhance the graph-based representation learning and make recommendations collectively in a coupled framework. This raises two challenges, 1) the long-tail downstream information may also bias the auxiliary graph learning, and 2) the learned auxiliary graph may cause negative transfer to the original user-item bipartite graph. We innovatively propose a novel Meta Graph Learning framework for long-tail recommendation (MGL) for solving both challenges. The meta-learning strategy is introduced to the learning of an edge generator, which is first tuned to reconstruct a debiased item co-occurrence matrix, and then virtually evaluated on generating item relations for recommendation. Moreover, we propose a popularity-aware contrastive learning strategy to prevent negative transfer by aligning the confident head item representations with those of the learned auxiliary graph. Experiments on public datasets demonstrate that our proposed model significantly outperforms strong baselines for tail items without compromising the overall performance."
FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings,"This article aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. book crossing.",https://doi.org/10.1145/3495163,missing,2022,"['Cheng-Te Li', 'Cheng Hsu', 'Yang Zhang']",Research,"Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This article aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning-based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users’ and items’ attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness."
Using Stable Matching to Optimize the Balance between Accuracy and Diversity in Recommendation,a two-sided post-processing approach in which both user and item utilities are considered. Our goal is to maximize aggregate diversity while minimizing loss in recommendation accuracy. Amazon books,https://doi.org/10.1145/3340631.3394858,"Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization",2020,"['Farzad Eskandanian', 'Bamshad Mobasher']",Research,"Increasing aggregate diversity (or catalog coverage) is an important system-level objective in many recommendation domains where it may be desirable to mitigate the popularity bias and to improve the coverage of long-tail items in recommendations given to users. This is especially important in multistakeholder recommendation scenarios where it may be important to optimize utilities not just for the end user, but also for other stakeholders such as item sellers or producers who desire a fair representation of their items across recommendation lists produced by the system. Unfortunately, attempts to increase aggregate diversity often result in lower recommendation accuracy for end users. Thus, addressing this problem requires an approach that can effectively manage the trade-offs between accuracy and aggregate diversity. In this work, we propose a two-sided post-processing approach in which both user and item utilities are considered. Our goal is to maximize aggregate diversity while minimizing loss in recommendation accuracy. Our solution is a generalization of the Deferred Acceptance algorithm which was proposed as an efficient algorithm to solve the well-known stable matching problem. We prove that our algorithm results in a unique user-optimal stable match between items and users. Using three recommendation datasets, we empirically demonstrate the effectiveness of our approach in comparison to several baselines. In particular, our results show that the proposed solution is quite effective in increasing aggregate diversity and item-side utility while optimizing recommendation accuracy for end users."
Flatter Is Better: Percentile Transformations for Recommender Systems,we demonstrate that a lack of flatness in rating distributions is negatively correlated with recommendation performance. book crossing,https://doi.org/10.1145/3437910,missing,2021,"['Masoud Mansoury', 'Robin Burke', 'Bamshad Mobasher']",Research,"It is well known that explicit user ratings in recommender systems are biased toward high ratings and that users differ significantly in their usage of the rating scale. Implementers usually compensate for these issues through rating normalization or the inclusion of a user bias term in factorization models. However, these methods adjust only for the central tendency of users’ distributions. In this work, we demonstrate that a lack of flatness in rating distributions is negatively correlated with recommendation performance. We propose a rating transformation model that compensates for skew in the rating distribution as well as its central tendency by converting ratings into percentile values as a pre-processing step before recommendation generation. This transformation flattens the rating distribution, better compensates for differences in rating distributions, and improves recommendation performance. We also show that a smoothed version of this transformation can yield more intuitive results for users with very narrow rating distributions. A comprehensive set of experiments, with state-of-the-art recommendation algorithms in four real-world datasets, show improved ranking performance for these percentile transformations."
CADPP: An Effective Approach to Recommend Attentive and Diverse Long-tail Items,a novel long-tail item recommendation approach which is based on the multi-pointer co-attention mechanism and the determinant point process. amazon books,https://doi.org/10.1145/3486622.3493961,IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,2022,"['Shuai Tang', 'Xiaofeng Zhang']",Research,"As the long-tail items are widely seen in various recommendation related applications, e.g., E-commerce and music recommendation, the long-tail recommendation consequently becomes an important research issue attracting both academic and industrial attentions. Apparently, it is a very challenging practical issue and the corresponding key challenges to address this task is to find the long-tail items which best match users’ preferences but are sufficiently diverse to avoid recommending similar long-tail items. To address this issue, this paper proposes a novel long-tail item recommendation approach which is based on the multi-pointer co-attention mechanism and the determinant point process (abbreviated as CADPP). Specifically, we design the multi-pointer co-attention mechanism for extracting important feature embeddings to capture the common characteristics of multiple items clicked by the users. We also employ the determinant point process (DPP) to allow diverse long-tail items but are relevant to the target items. To evaluate the model performance, extensive experiments have been performed on two real-world datasets. The promising results have demonstrated that the proposed CADPP is superior to both baseline and the state-of-the-art approaches with respect to the widely adopted evaluation metrics."
Empowering Long-tail Item Recommendation through Cross Decoupling Network (CDN),improve tail item recommendations while maintaining the overall performance with less training and serving cost. book crossing,https://doi.org/10.1145/3580305.3599814,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2023,"['Yin Zhang', 'Ruoxi Wang', 'Derek Cheng', 'Tiansheng Yao', 'Xinyang Yi', 'Lichan Hong', 'James Caverlee', 'Ed Chi']",Research,"Industry recommender systems usually suffer from highly-skewed long-tail item distributions where a small fraction of the items receives most of the user feedback. This skew hurts recommender quality especially for the item slices without much user feedback. While there have been many research advances made in academia, deploying these methods in production is very difficult and very few improvements have been made in industry. One challenge is that these methods often hurt overall performance; additionally, they could be complex and expensive to train and serve.In this work, we aim to improve tail item recommendations while maintaining the overall performance with less training and serving cost. We first find that the predictions of user preferences are biased under long-tail distributions. The bias comes from the differences between training and serving data in two perspectives: 1) the item distributions, and 2) user's preference given an item. Most existing methods mainly attempt to reduce the bias from the item distribution perspective, ignoring the discrepancy from user preference given an item. This leads to a severe forgetting issue and results in sub-optimal performance.To address the problem, we design a novel Cross Decoupling Network (CDN) to reduce the two differences. Specifically, CDN (i) decouples the learning process of memorization and generalization on the item side through a mixture-of-expert architecture; (ii) decouples the user samples from different distributions through a regularized bilateral branch network. Finally, a new adapter is introduced to aggregate the decoupled vectors, and softly shift the training attention to tail items. Extensive experimental results show that CDN significantly outperforms state-of-the-art approaches on popular benchmark datasets. We also demonstrate its effectiveness by a case study of CDN in a large-scale recommendation system at Google."
Exploring author gender in book rating and recommendation,"we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to a dimension of social concern, namely content creator gender. bookcrossing, amazonbooks",https://doi.org/10.1145/3240323.3240373,Proceedings of the 12th ACM Conference on Recommender Systems,2018,"['Michael Ekstrand', 'Mucun Tian', 'Mohammed Kazi', 'Hoda Mehrpouyan', 'Daniel Kluver']",Research,"Collaborative filtering algorithms find useful patterns in rating and consumption data and exploit these patterns to guide users to good items. Many of the patterns in rating datasets reflect important real-world differences between the various users and items in the data; other patterns may be irrelevant or possibly undesirable for social or ethical reasons, particularly if they reflect undesired discrimination, such as gender or ethnic discrimination in publishing. In this work, we examine the response of collaborative filtering recommender algorithms to the distribution of their input data with respect to a dimension of social concern, namely content creator gender. Using publicly-available book ratings data, we measure the distribution of the genders of the authors of books in user rating profiles and recommendation lists produced from this data. We find that common collaborative filtering algorithms differ in the gender distribution of their recommendation lists, and in the relationship of that output distribution to user profile distribution."
Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective,This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. goodreads,https://doi.org/10.1145/3580305.3599487,Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2023,"['Teng Xiao', 'Zhengyu Chen', 'Suhang Wang']",Research,"This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distribution shift, we further propose a principled framework, Adversarial Self-Training (AST), for unbiased recommendation. Extensive experiments on real-world and semi-synthetic datasets demonstrate the effectiveness of AST."
Power of the Few: Analyzing the Impact of Influential Users in Collaborative Recommender Systems,We then empirically identify and characterize influential users and analyze their impact on the system under different underlying recommendation algorithms and across three different recommendation domains. bookcrossing,https://doi.org/10.1145/3320435.3320464,"Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization",2019,"['Farzad Eskandanian', 'Nasim Sonboli', 'Bamshad Mobasher']",Research,"Like other social systems, in collaborative filtering a small number of ""influential"" users may have a large impact on the recommendations of other users, thus affecting the overall behavior of the system. Identifying influential users and studying their impact on other users is an important problem because it provides insight into how small groups can inadvertently or intentionally affect the behavior of the system as a whole. Modeling these influences can also shed light on patterns and relationships that would otherwise be difficult to discern, hopefully leading to more transparency in how the system generates personalized content. In this work we first formalize the notion of ""influence"" in collaborative filtering using an Influence Discrimination Model. We then empirically identify and characterize influential users and analyze their impact on the system under different underlying recommendation algorithms and across three different recommendation domains: job, movie and book recommendations. Insights from these experiments can help in designing systems that are not only optimized for accuracy, but are also tuned to mitigate the impact of influential users when it might lead to potential imbalance or unfairness in the system's outcomes."
Alleviating Cold-Start Problems in Recommendation through Pseudo-Labelling over Knowledge Graph,"such an optimisation strategy can lead to biased results toward already popular items by frequently handling new items as negative instances. In this study, we tackle the cold-start problems for new users/items by appropriately leveraging unobserved samples. bookcrossing",https://doi.org/10.1145/3437963.3441773,Proceedings of the 14th ACM International Conference on Web Search and Data Mining,2021,"['Riku Togashi', 'Mayu Otani', ""Shin'ichi Satoh""]",Research,"Solving cold-start problems is indispensable to provide meaningful recommendation results for new users and items. Under sparsely observed data, unobserved user-item pairs are also a vital source for distilling latent users' information needs. Most present works leverage unobserved samples for extracting negative signals. However, such an optimisation strategy can lead to biased results toward already popular items by frequently handling new items as negative instances. In this study, we tackle the cold-start problems for new users/items by appropriately leveraging unobserved samples. We propose a knowledge graph (KG)-aware recommender based on graph neural networks, which augments labelled samples through pseudo-labelling. Our approach aggressively employs unobserved samples as positive instances and brings new items into the spotlight. To avoid exhaustive label assignments to all possible pairs of users and items, we exploit a KG for selecting probably positive items for each user. We also utilise an improved negative sampling strategy and thereby suppress the exacerbation of popularity biases. Through experiments, we demonstrate that our approach achieves improvements over the state-of-the-art KG-aware recommenders in a variety of scenarios; in particular, our methodology successfully improves recommendation performance for cold-start users/items."
A Model of Two Tales: Dual Transfer Learning Framework for Improved Long-tail Item Recommendation,"to improve tail-item recommendation, we conduct research to transfer knowledge from head items to tail items, leveraging the rich user feedback in head items and the semantic connections between head and tail items. Bookcrossing",https://doi.org/10.1145/3442381.3450086,Proceedings of the Web Conference 2021,2021,"['Yin Zhang', 'Derek Cheng', 'Tiansheng Yao', 'Xinyang Yi', 'Lichan Hong', 'Ed Chi']",Research,"Highly skewed long-tail item distribution is very common in recommendation systems. It significantly hurts model performance on tail items. To improve tail-item recommendation, we conduct research to transfer knowledge from head items to tail items, leveraging the rich user feedback in head items and the semantic connections between head and tail items. Specifically, we propose a novel dual transfer learning framework that jointly learns the knowledge transfer from both model-level and item-level: 1. The model-level knowledge transfer builds a generic meta-mapping of model parameters from few-shot to many-shot model. It captures the implicit data augmentation on the model-level to improve the representation learning of tail items. 2. The item-level transfer connects head and tail items through item-level features, to ensure a smooth transfer of meta-mapping from head items to tail items. The two types of transfers are incorporated to ensure the learned knowledge from head items can be well applied for tail item representation learning in the long-tail distribution settings. Through extensive experiments on two benchmark datasets, results show that our proposed dual transfer learning framework significantly outperforms other state-of-the-art methods for tail item recommendation in hit ratio and NDCG. It is also very encouraging that our framework further improves head items and overall performance on top of the gains on tail items."
A Novel Classification Framework for Evaluating Individual and Aggregate Diversity in Top-N Recommendations,"It is found that in contradiction to common assumptions, not all users suffer as expected from the data sparsity problem. In fact, the group of users that receive the most accurate recommendations do not belong to the least sparse area of the dataset. bookcrossing",https://doi.org/10.1145/2700491,missing,2016,"['Jennifer Moody', 'David Glass']",Research,"The primary goal of a recommender system is to generate high quality user-centred recommendations. However, the traditional evaluation methods and metrics were developed before researchers understood all the factors that increase user satisfaction. This study is an introduction to a novel user and item classification framework. It is proposed that this framework should be used during user-centred evaluation of recommender systems and the need for this framework is justified through experiments. User profiles are constructed and matched against other users’ profiles to formulate neighbourhoods and generate top-N recommendations. The recommendations are evaluated to measure the success of the process. In conjunction with the framework, a new diversity metric is presented and explained. The accuracy, coverage, and diversity of top-N recommendations is illustrated and discussed for groups of users. It is found that in contradiction to common assumptions, not all users suffer as expected from the data sparsity problem. In fact, the group of users that receive the most accurate recommendations do not belong to the least sparse area of the dataset."
Measuring Fairness in Ranked Results: An Analytical and Empirical Comparison,"we describe several fair ranking metrics from the existing literature in a common notation, enabling direct comparison of their approaches and assumptions, and empirically compare them on the same experimental setup and data sets in the context of three information access tasks. goodreads",https://doi.org/10.1145/3477495.3532018,Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,2022,"['Amifa Raj', 'Michael Ekstrand']",Research,"Information access systems, such as search and recommender systems, often use ranked lists to present results believed to be relevant to the user's information need. Evaluating these lists for their fairness along with other traditional metrics provides a more complete understanding of an information access system's behavior beyond accuracy or utility constructs. To measure the (un)fairness of rankings, particularly with respect to the protected group(s) of producers or providers, several metrics have been proposed in the last several years. However, an empirical and comparative analyses of these metrics showing the applicability to specific scenario or real data, conceptual similarities, and differences is still lacking. We aim to bridge the gap between theoretical and practical ap-plication of these metrics. In this paper we describe several fair ranking metrics from the existing literature in a common notation, enabling direct comparison of their approaches and assumptions, and empirically compare them on the same experimental setup and data sets in the context of three information access tasks. We also provide a sensitivity analysis to assess the impact of the design choices and parameter settings that go in to these metrics and point to additional work needed to improve fairness measurement."
Enhancing Fairness in Meta-learned User Modeling via Adaptive Sampling,"Through the theoretical analysis that integrates the meta-learning paradigm with group fairness metrics, we identify group proportion imbalance as a critical factor. Subsequently, in order to mitigate the impact of this factor, we introduce a novel F airness-aware Adaptive S ampling framework for me T a-learning. bookccrossing",https://doi.org/10.1145/3589334.3645369,Proceedings of the ACM Web Conference 2024,2024,"['Zheng Zhang', 'Qi Liu', 'Zirui Hu', 'Yi Zhan', 'Zhenya Huang', 'Weibo Gao', 'Qingyang Mao']",Research,"Meta-learning has been widely employed to tackle the cold-start problem in user modeling. Similar to a guidebook for a new traveler, meta-learning significantly affects decision-making for new users in crucial scenarios, such as career recommendations. Consequently, the issue of fairness in meta-learning has gained paramount importance. Several methods have been proposed to mitigate unfairness in meta-learning and have shown promising results. However, a fundamental question remains unexplored: What is the critical factor leading to unfairness in meta-learned user modeling? Through the theoretical analysis that integrates the meta-learning paradigm with group fairness metrics, we identify group proportion imbalance as a critical factor. Subsequently, in order to mitigate the impact of this factor, we introduce a novel Fairness-aware Adaptive Sampling framework for meTa-learning, abbreviated as FAST. Its core concept involves adaptively adjusting the sampling distribution for different user groups during the interleaved training process of meta-learning. Furthermore, we provide theoretical guarantees demonstrating the convergence of FAST. Finally, empirical experiments conducted on three datasets reveal that FAST effectively enhances fairness while maintaining high accuracy. The code for FAST is available at https://github.com/zhengz99/FAST."
Estimation of Fair Ranking Metrics with Incomplete Judgments,"the protected attributes of individuals are rarely present, limiting the application of fair ranking metrics in large scale systems. In order to address this problem, we propose a sampling strategy and estimation technique for four fair ranking metrics. ekstrand's data",https://doi.org/10.1145/3442381.3450080,Proceedings of the Web Conference 2021,2021,"['\\""{O}mer K\\i{}rnap', 'Fernando Diaz', 'Asia Biega', 'Michael Ekstrand', 'Ben Carterette', 'Emine Yilmaz']",Research,"There is increasing attention to evaluating the fairness of search system ranking decisions. These metrics often consider the membership of items to particular groups, often identified using protected attributes such as gender or ethnicity. To date, these metrics typically assume the availability and completeness of protected attribute labels of items. However, the protected attributes of individuals are rarely present, limiting the application of fair ranking metrics in large scale systems. In order to address this problem, we propose a sampling strategy and estimation technique for four fair ranking metrics. We formulate a robust and unbiased estimator which can operate even with very limited number of labeled items. We evaluate our approach using both simulated and real world data. Our experimental results demonstrate that our method can estimate this family of fair ranking metrics and provides a robust, reliable alternative to exhaustive or random data annotation."
